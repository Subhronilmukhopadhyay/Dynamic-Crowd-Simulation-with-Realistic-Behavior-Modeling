{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Should print True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_1509/355509994.py:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  obsmat_df = pd.read_csv(\"ewap_dataset/seq_eth/obsmat.txt\", sep='\\s+', header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[8908, 4], edge_index=[2, 66418], batch=[8908], ptr=[1449])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv  # Example GCNConv\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load and prepare data\n",
    "obsmat_df = pd.read_csv(\"ewap_dataset/seq_eth/obsmat.txt\", sep='\\s+', header=None)\n",
    "obsmat_df.columns = ['frame_number', 'pedestrian_ID', 'pos_x', 'pos_z', 'pos_y', 'v_x', 'v_z', 'v_y']\n",
    "obsmat_df.drop(['pos_z', 'v_z'], axis=1, inplace=True)\n",
    "\n",
    "# Compute mean and std over the entire dataset for positions and velocities.\n",
    "mean = obsmat_df[['pos_x', 'pos_y', 'v_x', 'v_y']].mean()\n",
    "std = obsmat_df[['pos_x', 'pos_y', 'v_x', 'v_y']].std()\n",
    "\n",
    "# Normalize the columns.\n",
    "obsmat_df[['pos_x', 'pos_y', 'v_x', 'v_y']] = (obsmat_df[['pos_x', 'pos_y', 'v_x', 'v_y']] - mean) / std\n",
    "\n",
    "# 2. Graph creation function\n",
    "def create_graph(frame_data, k=2):\n",
    "    positions = frame_data[['pos_x', 'pos_y']].values\n",
    "    velocities = frame_data[['v_x', 'v_y']].values\n",
    "    # pedestrian = frame_data['pedestrian_ID'].values\n",
    "    features = np.concatenate([positions, velocities], axis=1)\n",
    "    num_pedestrians = len(positions)\n",
    "\n",
    "    if num_pedestrians > 1:\n",
    "        knn = NearestNeighbors(n_neighbors=max(k, num_pedestrians - 1))\n",
    "        knn.fit(positions)\n",
    "        _, indices = knn.kneighbors(positions)\n",
    "\n",
    "        edge_index = []\n",
    "        for i in range(len(positions)):\n",
    "            for j in indices[i]:\n",
    "                if i != j:\n",
    "                    # edge_index.append([pedestrian[i], pedestrian[j]])\n",
    "                    edge_index.append([i, j])\n",
    "\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        x = torch.tensor(features, dtype=torch.float)\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        return data\n",
    "    elif num_pedestrians == 1:\n",
    "        x = torch.tensor(features, dtype=torch.float)\n",
    "        # edge_index = torch.tensor([[pedestrian[0]], [pedestrian[0]]], dtype=torch.long)\n",
    "        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "        data = Data(x=x, edge_index=edge_index)  # self edge_index if only one pedestrian.\n",
    "        return data\n",
    "    else: # if there is no pedestrian in the frame\n",
    "      return None\n",
    "\n",
    "\n",
    "# 3. Create graphs for all frames and batch them\n",
    "graph_list = []\n",
    "frames = obsmat_df.groupby('frame_number')\n",
    "for frame_id, frame_number in frames:\n",
    "    graph = create_graph(frame_number)\n",
    "    if(graph is not None):\n",
    "        graph_list.append(graph)\n",
    "\n",
    "# for data in graph_list:\n",
    "#     if not hasattr(data, 'edge_index'):\n",
    "#         data.edge_index = torch.tensor([[], []], dtype=torch.long)\n",
    "\n",
    "# filtered_graph_list = [data for data in graph_list if hasattr(data, 'edge_index')]\n",
    "# data_batch = Batch.from_data_list(graph_list)\n",
    "# print(data_batch)\n",
    "\n",
    "data_batch = Batch.from_data_list(graph_list) #This is the batched data ready for the GNN\n",
    "print(data_batch)\n",
    "\n",
    "# 4. Feature Scaling (Important!)\n",
    "# Calculate mean and std *across all frames* for each feature\n",
    "feature_means = data_batch.x.mean(dim=0)\n",
    "feature_stds = data_batch.x.std(dim=0)\n",
    "\n",
    "# Normalize the features in the batched graph\n",
    "data_batch.x = (data_batch.x - feature_means) / (feature_stds + 1e-7)  # Adding a small epsilon for numerical stability\n",
    "\n",
    "\n",
    "# Now 'data_batch' is ready to be passed to your GNN model.\n",
    "# Example usage (assuming you have a 'model' defined):\n",
    "# output = model(data_batch.x, data_batch.edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #trail to check for work\n",
    "\n",
    "# frame_data = obsmat_df.groupby('frame_number').get_group(834)\n",
    "\n",
    "# # print(f\"First Frame ID: {first_frame_id}\")\n",
    "# print(\"Frame Data:\")\n",
    "# # print(frame_data)\n",
    "\n",
    "# positions = frame_data[['pos_x', 'pos_y']].values\n",
    "# velocities = frame_data[['v_x', 'v_y']].values\n",
    "# features = np.concatenate([positions, velocities], axis=1)\n",
    "# pedestrian_ids = frame_data['pedestrian_ID'].values  # Get pedestrian IDs\n",
    "# num_pedestrians = len(positions)\n",
    "\n",
    "# if num_pedestrians > 1:\n",
    "#     knn = NearestNeighbors(n_neighbors=max(2, num_pedestrians - 1))\n",
    "#     knn.fit(positions)\n",
    "#     _, indices = knn.kneighbors(positions)\n",
    "\n",
    "#     node_indices = {ped_id: i for i, ped_id in enumerate(pedestrian_ids)}\n",
    "#     edge_index = []\n",
    "#     for i, ped_id_i in enumerate(pedestrian_ids):\n",
    "#         for j_idx in indices[i]:\n",
    "#             ped_id_j = pedestrian_ids[j_idx]\n",
    "#             if ped_id_i != ped_id_j:\n",
    "#                 edge_index.append([node_indices[ped_id_i], node_indices[ped_id_j]])\n",
    "\n",
    "#     edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "#     x = torch.tensor(features, dtype=torch.float)\n",
    "#     data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# # data.edge_index\n",
    "# edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[11, 4], edge_index=[2, 99]),\n",
       " Data(x=[11, 4], edge_index=[2, 99]),\n",
       " Data(x=[11, 4], edge_index=[2, 99]),\n",
       " Data(x=[11, 4], edge_index=[2, 99]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[11, 4], edge_index=[2, 99]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[11, 4], edge_index=[2, 99]),\n",
       " Data(x=[11, 4], edge_index=[2, 99]),\n",
       " Data(x=[11, 4], edge_index=[2, 99]),\n",
       " Data(x=[11, 4], edge_index=[2, 99]),\n",
       " Data(x=[11, 4], edge_index=[2, 99]),\n",
       " Data(x=[11, 4], edge_index=[2, 99]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[12, 4], edge_index=[2, 120]),\n",
       " Data(x=[12, 4], edge_index=[2, 120]),\n",
       " Data(x=[12, 4], edge_index=[2, 120]),\n",
       " Data(x=[12, 4], edge_index=[2, 120]),\n",
       " Data(x=[12, 4], edge_index=[2, 120]),\n",
       " Data(x=[12, 4], edge_index=[2, 120]),\n",
       " Data(x=[12, 4], edge_index=[2, 120]),\n",
       " Data(x=[12, 4], edge_index=[2, 120]),\n",
       " Data(x=[12, 4], edge_index=[2, 120]),\n",
       " Data(x=[11, 4], edge_index=[2, 99]),\n",
       " Data(x=[15, 4], edge_index=[2, 195]),\n",
       " Data(x=[15, 4], edge_index=[2, 195]),\n",
       " Data(x=[15, 4], edge_index=[2, 195]),\n",
       " Data(x=[15, 4], edge_index=[2, 195]),\n",
       " Data(x=[15, 4], edge_index=[2, 195]),\n",
       " Data(x=[15, 4], edge_index=[2, 195]),\n",
       " Data(x=[15, 4], edge_index=[2, 195]),\n",
       " Data(x=[15, 4], edge_index=[2, 195]),\n",
       " Data(x=[15, 4], edge_index=[2, 195]),\n",
       " Data(x=[15, 4], edge_index=[2, 195]),\n",
       " Data(x=[14, 4], edge_index=[2, 168]),\n",
       " Data(x=[13, 4], edge_index=[2, 143]),\n",
       " Data(x=[12, 4], edge_index=[2, 120]),\n",
       " Data(x=[11, 4], edge_index=[2, 99]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[4, 4], edge_index=[2, 8]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[11, 4], edge_index=[2, 99]),\n",
       " Data(x=[11, 4], edge_index=[2, 99]),\n",
       " Data(x=[11, 4], edge_index=[2, 99]),\n",
       " Data(x=[12, 4], edge_index=[2, 120]),\n",
       " Data(x=[12, 4], edge_index=[2, 120]),\n",
       " Data(x=[12, 4], edge_index=[2, 120]),\n",
       " Data(x=[12, 4], edge_index=[2, 120]),\n",
       " Data(x=[13, 4], edge_index=[2, 143]),\n",
       " Data(x=[13, 4], edge_index=[2, 143]),\n",
       " Data(x=[13, 4], edge_index=[2, 143]),\n",
       " Data(x=[13, 4], edge_index=[2, 143]),\n",
       " Data(x=[13, 4], edge_index=[2, 143]),\n",
       " Data(x=[13, 4], edge_index=[2, 143]),\n",
       " Data(x=[14, 4], edge_index=[2, 168]),\n",
       " Data(x=[13, 4], edge_index=[2, 143]),\n",
       " Data(x=[13, 4], edge_index=[2, 143]),\n",
       " Data(x=[13, 4], edge_index=[2, 143]),\n",
       " Data(x=[13, 4], edge_index=[2, 143]),\n",
       " Data(x=[14, 4], edge_index=[2, 168]),\n",
       " Data(x=[15, 4], edge_index=[2, 195]),\n",
       " Data(x=[15, 4], edge_index=[2, 195]),\n",
       " Data(x=[15, 4], edge_index=[2, 195]),\n",
       " Data(x=[15, 4], edge_index=[2, 195]),\n",
       " Data(x=[15, 4], edge_index=[2, 195]),\n",
       " Data(x=[13, 4], edge_index=[2, 143]),\n",
       " Data(x=[12, 4], edge_index=[2, 120]),\n",
       " Data(x=[10, 4], edge_index=[2, 80]),\n",
       " Data(x=[9, 4], edge_index=[2, 63]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[8, 4], edge_index=[2, 48]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[7, 4], edge_index=[2, 35]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[6, 4], edge_index=[2, 24]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[5, 4], edge_index=[2, 15]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " Data(x=[3, 4], edge_index=[2, 3]),\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "print(graph_list[1].edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 1. Define the Models\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) GNN Model: Processes a graph per frame.\n",
    "class PedestrianGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, heads=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): Dimension of input node features (e.g. 4 for [pos_x, pos_y, v_x, v_y]).\n",
    "            hidden_dim (int): Hidden dimension for the GCN layers.\n",
    "            output_dim (int): Output dimension; should match the transformer’s d_model.\n",
    "        \"\"\"\n",
    "        super(PedestrianGNN, self).__init__()\n",
    "        self.gat1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=0.1)\n",
    "        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, dropout=0.1)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "\n",
    "        x = F.elu(self.gat1(x, edge_index))\n",
    "        x = F.elu(self.gat2(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Transformer with positional encoding.\n",
    "class TrajectoryTransformerWithPE(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dropout=0.1):\n",
    "        super(TrajectoryTransformerWithPE, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, 2)\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        transformer_output = self.transformer(src, tgt)\n",
    "        output = self.fc_out(transformer_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################\n",
    "# 2. Define the Dataset and Collate Function\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, graph_list, input_seq_len, target_seq_len):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            graph_list (list): List of torch_geometric.data.Data objects, one per frame.\n",
    "            input_seq_len (int): Number of consecutive frames used as input.\n",
    "            target_seq_len (int): Number of consecutive frames to predict.\n",
    "        \"\"\"\n",
    "        self.graph_list = graph_list\n",
    "        self.input_seq_len = input_seq_len\n",
    "        self.target_seq_len = target_seq_len\n",
    "        self.total_seq_len = input_seq_len + target_seq_len\n",
    "        if len(self.graph_list) < self.total_seq_len:\n",
    "            raise ValueError(\"Not enough frames for the given sequence lengths.\")\n",
    "    def __len__(self):\n",
    "        return len(self.graph_list) - self.total_seq_len + 1\n",
    "    def pool_frame(self, frame):\n",
    "        # Pool the positions (columns 0 and 1) from all nodes.\n",
    "        return frame.x[:, :2].mean(dim=0)\n",
    "    def __getitem__(self, idx):\n",
    "        input_frames = self.graph_list[idx: idx + self.input_seq_len]\n",
    "        target_frames = self.graph_list[idx + self.input_seq_len: idx + self.total_seq_len]\n",
    "        tgt_positions = torch.stack([self.pool_frame(frame) for frame in target_frames], dim=0)\n",
    "        return input_frames, tgt_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_frames_batch = [item[0] for item in batch]\n",
    "    tgt_positions_batch = torch.stack([item[1] for item in batch], dim=0)\n",
    "    return input_frames_batch, tgt_positions_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 3. Hyperparameters, Dataset, and DataLoader\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sequence lengths (adjust as needed)\n",
    "input_seq_len = 8\n",
    "target_seq_len = 12\n",
    "\n",
    "# Create dataset and dataloader using the actual BIWI data graphs.\n",
    "dataset = TrajectoryDataset(graph_list, input_seq_len, target_seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters.\n",
    "input_dim = 4        # [pos_x, pos_y, v_x, v_y]\n",
    "hidden_dim = 64\n",
    "d_model = 64         # Should equal GNN output dimension.\n",
    "nhead = 8\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 4. Instantiate Models, Optimizer, and Loss Function\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate models.\n",
    "gnn_model = PedestrianGNN(input_dim, hidden_dim, d_model, heads=4).to(device)\n",
    "transformer_model = TrajectoryTransformerWithPE(d_model, nhead, num_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder input mapping: maps 2D target positions to d_model.\n",
    "decoder_input_mapping = nn.Linear(2, d_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subhronil/anaconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(list(gnn_model.parameters()) +\n",
    "                       list(transformer_model.parameters()) +\n",
    "                       list(decoder_input_mapping.parameters()),\n",
    "                       lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 5. The Training Loop\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    gnn_model.train()\n",
    "    transformer_model.train()\n",
    "    decoder_input_mapping.train()\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, (input_frames_batch, tgt_positions_batch) in enumerate(dataloader):\n",
    "        B = len(input_frames_batch)\n",
    "        src_embeddings_batch = []  # To hold the sequence embeddings for each sample.\n",
    "        \n",
    "        # Process each sample in the batch.\n",
    "        for sample in input_frames_batch:\n",
    "            sample_embeddings = []\n",
    "            for frame in sample:\n",
    "                x = frame.x.to(device)\n",
    "                edge_index = frame.edge_index.to(device)\n",
    "                # Create a batch vector for a single graph (all nodes belong to graph 0).\n",
    "                batch_vector = torch.zeros(x.size(0), dtype=torch.long).to(device)\n",
    "                emb = gnn_model(x, edge_index, batch_vector)  # [1, d_model]\n",
    "                sample_embeddings.append(emb.squeeze(0))       # [d_model]\n",
    "            sample_seq = torch.stack(sample_embeddings, dim=0)  # [input_seq_len, d_model]\n",
    "            src_embeddings_batch.append(sample_seq)\n",
    "        \n",
    "        src_embeddings = torch.stack(src_embeddings_batch, dim=0)  # [B, input_seq_len, d_model]\n",
    "        \n",
    "        # Prepare decoder inputs with teacher forcing:\n",
    "        # Map ground-truth target positions (except last time step) to d_model space.\n",
    "        decoder_inputs_mapped = decoder_input_mapping(tgt_positions_batch[:, :-1, :].to(device))  # [B, target_seq_len-1, d_model]\n",
    "        start_token = torch.zeros(B, 1, d_model, device=device)\n",
    "        decoder_inputs = torch.cat([start_token, decoder_inputs_mapped], dim=1)  # [B, target_seq_len, d_model]\n",
    "        \n",
    "        # Forward pass through the Transformer.\n",
    "        output = transformer_model(src_embeddings, decoder_inputs)  # [B, target_seq_len, 2]\n",
    "        \n",
    "        loss = criterion(output, tgt_positions_batch.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    scheduler.step(avg_loss)\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#############################################################################\n",
    "\n",
    "JUST FOR INFORMATION: THE ACTUAL LOSS AND VALUE WAS TAKEN FROM THE BELOW CODE\n",
    "\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1209\n",
      "Epoch 2/10, Loss: 0.0563\n",
      "Epoch 3/10, Loss: 0.0460\n",
      "Epoch 4/10, Loss: 0.0425\n",
      "Epoch 5/10, Loss: 0.0377\n",
      "Epoch 6/10, Loss: 0.0351\n",
      "Epoch 7/10, Loss: 0.0351\n",
      "Epoch 8/10, Loss: 0.0328\n",
      "Epoch 9/10, Loss: 0.0294\n",
      "Epoch 10/10, Loss: 0.0317\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# # Improved GNN using GAT layers.\n",
    "# class PedestrianGNN(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim, heads=4):\n",
    "#         super(PedestrianGNN, self).__init__()\n",
    "#         self.gat1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=0.1)\n",
    "#         self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, dropout=0.1)\n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         x = F.elu(self.gat1(x, edge_index))\n",
    "#         x = F.elu(self.gat2(x, edge_index))\n",
    "#         x = global_mean_pool(x, batch)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# # Positional Encoding.\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "#         super(PositionalEncoding, self).__init__()\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n",
    "#         self.register_buffer('pe', pe)\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.pe[:, :x.size(1), :]\n",
    "#         return self.dropout(x)\n",
    "\n",
    "# # Transformer with positional encoding.\n",
    "# class TrajectoryTransformerWithPE(nn.Module):\n",
    "#     def __init__(self, d_model, nhead, num_layers, dropout=0.1):\n",
    "#         super(TrajectoryTransformerWithPE, self).__init__()\n",
    "#         self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "#         self.transformer = nn.Transformer(\n",
    "#             d_model=d_model,\n",
    "#             nhead=nhead,\n",
    "#             num_encoder_layers=num_layers,\n",
    "#             num_decoder_layers=num_layers,\n",
    "#             dropout=dropout,\n",
    "#             batch_first=True\n",
    "#         )\n",
    "#         self.fc_out = nn.Linear(d_model, 2)\n",
    "#     def forward(self, src, tgt):\n",
    "#         src = self.pos_encoder(src)\n",
    "#         tgt = self.pos_encoder(tgt)\n",
    "#         transformer_output = self.transformer(src, tgt)\n",
    "#         output = self.fc_out(transformer_output)\n",
    "#         return output\n",
    "\n",
    "# ##############################################\n",
    "# # 3. Trajectory Dataset and Collate Function\n",
    "# ##############################################\n",
    "\n",
    "# class TrajectoryDataset(Dataset):\n",
    "#     def __init__(self, graph_list, input_seq_len, target_seq_len):\n",
    "#         self.graph_list = graph_list\n",
    "#         self.input_seq_len = input_seq_len\n",
    "#         self.target_seq_len = target_seq_len\n",
    "#         self.total_seq_len = input_seq_len + target_seq_len\n",
    "#         if len(self.graph_list) < self.total_seq_len:\n",
    "#             raise ValueError(\"Not enough frames for the given sequence lengths.\")\n",
    "#     def __len__(self):\n",
    "#         return len(self.graph_list) - self.total_seq_len + 1\n",
    "#     def pool_frame(self, frame):\n",
    "#         # Pool the positions (columns 0 and 1) from all nodes.\n",
    "#         return frame.x[:, :2].mean(dim=0)\n",
    "#     def __getitem__(self, idx):\n",
    "#         input_frames = self.graph_list[idx: idx + self.input_seq_len]\n",
    "#         target_frames = self.graph_list[idx + self.input_seq_len: idx + self.total_seq_len]\n",
    "#         tgt_positions = torch.stack([self.pool_frame(frame) for frame in target_frames], dim=0)\n",
    "#         return input_frames, tgt_positions\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     input_frames_batch = [item[0] for item in batch]\n",
    "#     tgt_positions_batch = torch.stack([item[1] for item in batch], dim=0)\n",
    "#     return input_frames_batch, tgt_positions_batch\n",
    "\n",
    "# # Set sequence lengths (adjust as needed)\n",
    "# input_seq_len = 8\n",
    "# target_seq_len = 12\n",
    "\n",
    "# # Create dataset and dataloader using the actual BIWI data graphs.\n",
    "# dataset = TrajectoryDataset(graph_list, input_seq_len, target_seq_len)\n",
    "# dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# ##############################################\n",
    "# # 4. Initialize Models, Optimizer, and Loss Function\n",
    "# ##############################################\n",
    "\n",
    "# # Hyperparameters.\n",
    "# input_dim = 4        # [pos_x, pos_y, v_x, v_y]\n",
    "# hidden_dim = 64\n",
    "# d_model = 64         # Should equal GNN output dimension.\n",
    "# nhead = 8\n",
    "# num_layers = 2\n",
    "# dropout = 0.1\n",
    "# learning_rate = 1e-3\n",
    "# num_epochs = 10\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Instantiate models.\n",
    "# gnn_model = PedestrianGNN(input_dim, hidden_dim, d_model, heads=4).to(device)\n",
    "# transformer_model = TrajectoryTransformerWithPE(d_model, nhead, num_layers, dropout).to(device)\n",
    "# # Decoder input mapping: maps 2D target positions to d_model.\n",
    "# decoder_input_mapping = nn.Linear(2, d_model).to(device)\n",
    "\n",
    "# optimizer = optim.Adam(list(gnn_model.parameters()) +\n",
    "#                        list(transformer_model.parameters()) +\n",
    "#                        list(decoder_input_mapping.parameters()),\n",
    "#                        lr=learning_rate)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# ##############################################\n",
    "# # 5. Training Loop\n",
    "# ##############################################\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     gnn_model.train()\n",
    "#     transformer_model.train()\n",
    "#     decoder_input_mapping.train()\n",
    "    \n",
    "#     epoch_loss = 0.0\n",
    "#     for batch_idx, (input_frames_batch, tgt_positions_batch) in enumerate(dataloader):\n",
    "#         B = len(input_frames_batch)\n",
    "#         src_embeddings_batch = []  # To hold the sequence embeddings for each sample.\n",
    "        \n",
    "#         # Process each sample in the batch.\n",
    "#         for sample in input_frames_batch:\n",
    "#             sample_embeddings = []\n",
    "#             for frame in sample:\n",
    "#                 x = frame.x.to(device)\n",
    "#                 edge_index = frame.edge_index.to(device)\n",
    "#                 # Create a batch vector for a single graph (all nodes belong to graph 0).\n",
    "#                 batch_vector = torch.zeros(x.size(0), dtype=torch.long).to(device)\n",
    "#                 emb = gnn_model(x, edge_index, batch_vector)  # [1, d_model]\n",
    "#                 sample_embeddings.append(emb.squeeze(0))       # [d_model]\n",
    "#             sample_seq = torch.stack(sample_embeddings, dim=0)  # [input_seq_len, d_model]\n",
    "#             src_embeddings_batch.append(sample_seq)\n",
    "        \n",
    "#         src_embeddings = torch.stack(src_embeddings_batch, dim=0)  # [B, input_seq_len, d_model]\n",
    "        \n",
    "#         # Prepare decoder inputs with teacher forcing:\n",
    "#         # Map ground-truth target positions (except last time step) to d_model space.\n",
    "#         decoder_inputs_mapped = decoder_input_mapping(tgt_positions_batch[:, :-1, :].to(device))  # [B, target_seq_len-1, d_model]\n",
    "#         start_token = torch.zeros(B, 1, d_model, device=device)\n",
    "#         decoder_inputs = torch.cat([start_token, decoder_inputs_mapped], dim=1)  # [B, target_seq_len, d_model]\n",
    "        \n",
    "#         # Forward pass through the Transformer.\n",
    "#         output = transformer_model(src_embeddings, decoder_inputs)  # [B, target_seq_len, 2]\n",
    "        \n",
    "#         loss = criterion(output, tgt_positions_batch.to(device))\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         epoch_loss += loss.item()\n",
    "#     scheduler.step(avg_loss)\n",
    "#     avg_loss = epoch_loss / len(dataloader)\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming these are your trained model instances:\n",
    "# gnn_model, transformer_model, decoder_input_mapping\n",
    "# And your optimizer is 'optimizer'\n",
    "\n",
    "# Save checkpoint after training\n",
    "checkpoint = {\n",
    "    'epoch': num_epochs,\n",
    "    'gnn_state_dict': gnn_model.state_dict(),\n",
    "    'transformer_state_dict': transformer_model.state_dict(),\n",
    "    'decoder_input_mapping_state_dict': decoder_input_mapping.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': avg_loss,\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'trajectory_model_checkpoint.pth')\n",
    "print(\"Model checkpoint saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1509/3769514250.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('trajectory_model_checkpoint.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 10 with loss 0.0317\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Re-create your model architectures with the same parameters\n",
    "gnn_model = PedestrianGNN(input_dim, hidden_dim, d_model).to(device)\n",
    "transformer_model = TrajectoryTransformerWithPE(d_model=d_model, nhead=nhead, num_layers=num_layers, dropout=dropout).to(device)\n",
    "decoder_input_mapping = nn.Linear(2, d_model).to(device)\n",
    "\n",
    "optimizer = optim.Adam(list(gnn_model.parameters()) +\n",
    "                       list(transformer_model.parameters()) +\n",
    "                       list(decoder_input_mapping.parameters()),\n",
    "                       lr=learning_rate)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load('trajectory_model_checkpoint.pth')\n",
    "gnn_model.load_state_dict(checkpoint['gnn_state_dict'])\n",
    "transformer_model.load_state_dict(checkpoint['transformer_state_dict'])\n",
    "decoder_input_mapping.load_state_dict(checkpoint['decoder_input_mapping_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "start_epoch = checkpoint['epoch']\n",
    "print(f\"Loaded model from epoch {start_epoch} with loss {checkpoint['loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Trajectories: tensor([[[-0.0679, -0.0402],\n",
      "         [-0.0675, -0.0460],\n",
      "         [-0.0523, -0.0518],\n",
      "         [-0.0426, -0.0495],\n",
      "         [-0.0363, -0.0457],\n",
      "         [-0.0348, -0.0424],\n",
      "         [-0.0377, -0.0439],\n",
      "         [-0.0381, -0.0460],\n",
      "         [-0.0373, -0.0508],\n",
      "         [-0.0368, -0.0490],\n",
      "         [-0.0221, -0.0467],\n",
      "         [-0.0081, -0.0435]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Set models to evaluation mode\n",
    "gnn_model.eval()\n",
    "transformer_model.eval()\n",
    "decoder_input_mapping.eval()\n",
    "\n",
    "# --- Step 1: Prepare Test Input ---\n",
    "# Use an existing test graph from your graph_list.\n",
    "test_graph = graph_list[0]  # Using the first graph as an example\n",
    "\n",
    "# Move test graph data to the correct device\n",
    "test_graph.x = test_graph.x.to(device)\n",
    "test_graph.edge_index = test_graph.edge_index.to(device)\n",
    "\n",
    "# --- Step 2: Get GNN Embeddings from the Test Graph ---\n",
    "with torch.no_grad():\n",
    "    # Create a dummy batch vector for the test graph's nodes on the device\n",
    "    dummy_batch = torch.zeros(test_graph.x.shape[0], dtype=torch.long, device=device)\n",
    "    # Pass test graph through GNN to get node embeddings\n",
    "    test_gnn_output = gnn_model(test_graph.x, test_graph.edge_index, dummy_batch)  # Now provides expected output\n",
    "    # Treat the nodes as a sequence (if your Transformer expects a sequence, add a batch dimension)\n",
    "    test_src = test_gnn_output.unsqueeze(0)  # Shape: (1, num_nodes, d_model)\n",
    "\n",
    "# --- Step 3: Prepare Decoder Input for the Transformer ---\n",
    "dummy_decoder_input = torch.zeros(1, target_seq_len, 2, device=device)\n",
    "decoder_inputs = decoder_input_mapping(dummy_decoder_input)  # Shape: (1, target_seq_len, d_model)\n",
    "\n",
    "# --- Step 4: Run the Transformer to Get Predicted Trajectories ---\n",
    "with torch.no_grad():\n",
    "    predictions = transformer_model(test_src, decoder_inputs)\n",
    "    print(\"Predicted Trajectories:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def get_sample(sample_index, dataset, device):\n",
    "    \"\"\"\n",
    "    Returns the ground truth and predicted trajectories for a single sample.\n",
    "    \n",
    "    Args:\n",
    "        sample_index (int): Index into the dataset.\n",
    "        dataset (TrajectoryDataset): Your dataset built from BIWI data.\n",
    "        device (torch.device): Device (CPU or GPU) on which the models are.\n",
    "    \n",
    "    Returns:\n",
    "        true_positions (numpy array): Ground truth trajectory [target_seq_len, 2].\n",
    "        pred_positions (numpy array): Predicted trajectory [target_seq_len, 2].\n",
    "    \"\"\"\n",
    "    # Get a single sample from the dataset:\n",
    "    input_frames, tgt_positions = dataset[sample_index]\n",
    "    \n",
    "    # Process input frames with the GNN to get a sequence of embeddings:\n",
    "    frame_embeddings = []\n",
    "    for frame in input_frames:\n",
    "        x = frame.x.to(device)\n",
    "        edge_index = frame.edge_index.to(device)\n",
    "        # For a single graph, create a batch vector (all nodes belong to graph 0)\n",
    "        batch_vector = torch.zeros(x.size(0), dtype=torch.long).to(device)\n",
    "        node_embeddings = gnn_model(x, edge_index, batch_vector)  # [1, d_model]\n",
    "        # Here, we already have a graph-level embedding, so we simply squeeze it.\n",
    "        frame_emb = node_embeddings.squeeze(0)  # [d_model]\n",
    "        frame_embeddings.append(frame_emb)\n",
    "    \n",
    "    # Stack frame embeddings to form the source sequence for the Transformer.\n",
    "    src_embeddings = torch.stack(frame_embeddings, dim=0)  # [input_seq_len, d_model]\n",
    "    src_embeddings = src_embeddings.unsqueeze(0)  # [1, input_seq_len, d_model]\n",
    "    \n",
    "    # Prepare decoder input using teacher forcing:\n",
    "    # Convert ground-truth target positions (shape: [target_seq_len, 2]) to tensor\n",
    "    tgt_positions_tensor = tgt_positions.unsqueeze(0).to(device)  # [1, target_seq_len, 2]\n",
    "    # Map ground truth positions (except the last time step) into the Transformer embedding space.\n",
    "    decoder_inputs_mapped = decoder_input_mapping(tgt_positions_tensor[:, :-1, :])  # [1, target_seq_len-1, d_model]\n",
    "    # Create a start token (zeros)\n",
    "    start_token = torch.zeros(1, 1, src_embeddings.size(-1), device=device)\n",
    "    # Form the decoder input: [1, target_seq_len, d_model]\n",
    "    decoder_inputs = torch.cat([start_token, decoder_inputs_mapped], dim=1)\n",
    "    \n",
    "    # Get prediction from the Transformer.\n",
    "    with torch.no_grad():\n",
    "        pred_positions = transformer_model(src_embeddings, decoder_inputs)  # [1, target_seq_len, 2]\n",
    "    pred_positions = pred_positions.squeeze(0).cpu().numpy()  # [target_seq_len, 2]\n",
    "    true_positions = tgt_positions.cpu().numpy()              # [target_seq_len, 2]\n",
    "    \n",
    "    return true_positions, pred_positions\n",
    "\n",
    "# Choose a sample index from your dataset (e.g., 0)\n",
    "sample_index = 0\n",
    "true_positions, pred_positions = get_sample(sample_index, dataset, device)\n",
    "\n",
    "# Plot ground-truth vs. predicted trajectory:\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(true_positions[:, 0], true_positions[:, 1], 'o-', label=\"Ground Truth\", linewidth=2)\n",
    "plt.plot(pred_positions[:, 0], pred_positions[:, 1], 'x--', label=\"Prediction\", linewidth=2)\n",
    "plt.xlabel(\"X Position\")\n",
    "plt.ylabel(\"Y Position\")\n",
    "plt.title(\"Trajectory Prediction\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denormalized ADE: 0.5937\n",
      "Denormalized FDE: 0.2579\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def denormalize_positions(positions, mean_vals, std_vals):\n",
    "    \"\"\"\n",
    "    Denormalizes the positions using mean and std values.\n",
    "    \n",
    "    Args:\n",
    "        positions (numpy array): Shape [T, 2] in normalized units.\n",
    "        mean_vals (pandas Series or dict): Mean values for 'pos_x' and 'pos_y'.\n",
    "        std_vals (pandas Series or dict): Standard deviation values for 'pos_x' and 'pos_y'.\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: Denormalized positions in real-world units.\n",
    "    \"\"\"\n",
    "    # Extract mean and std for positions (pos_x, pos_y)\n",
    "    pos_mean = np.array([mean_vals['pos_x'], mean_vals['pos_y']])\n",
    "    pos_std = np.array([std_vals['pos_x'], std_vals['pos_y']])\n",
    "    \n",
    "    # Denormalize: original = normalized * std + mean\n",
    "    denorm_positions = positions * pos_std + pos_mean\n",
    "    return denorm_positions\n",
    "\n",
    "def compute_ADE_FDE(pred_positions, true_positions):\n",
    "    \"\"\"\n",
    "    Computes Average Displacement Error (ADE) and Final Displacement Error (FDE).\n",
    "    \n",
    "    Args:\n",
    "        pred_positions (numpy array): Predicted trajectory, shape [T, 2].\n",
    "        true_positions (numpy array): Ground truth trajectory, shape [T, 2].\n",
    "    \n",
    "    Returns:\n",
    "        ADE (float): Average displacement error.\n",
    "        FDE (float): Final displacement error.\n",
    "    \"\"\"\n",
    "    errors = np.linalg.norm(pred_positions - true_positions, axis=1)\n",
    "    ADE = errors.mean()\n",
    "    FDE = errors[-1]\n",
    "    return ADE, FDE\n",
    "\n",
    "# Example: assume get_sample returns normalized predictions and ground truth.\n",
    "true_positions_norm, pred_positions_norm = get_sample(sample_index, dataset, device)\n",
    "\n",
    "# Denormalize the positions:\n",
    "true_positions_denorm = denormalize_positions(true_positions_norm, mean, std)\n",
    "pred_positions_denorm = denormalize_positions(pred_positions_norm, mean, std)\n",
    "\n",
    "# Compute ADE and FDE:\n",
    "ADE, FDE = compute_ADE_FDE(pred_positions_denorm, true_positions_denorm)\n",
    "print(f\"Denormalized ADE: {ADE:.4f}\")\n",
    "print(f\"Denormalized FDE: {FDE:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ADE: 0.1441\n",
      "Average FDE: 0.2256\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(dataset, gnn_model, transformer_model, decoder_input_mapping, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given dataset and compute average ADE and FDE.\n",
    "    \n",
    "    Args:\n",
    "        dataset: An instance of your TrajectoryDataset.\n",
    "        gnn_model: The trained GNN model.\n",
    "        transformer_model: The trained Transformer model.\n",
    "        decoder_input_mapping: The linear layer mapping 2D positions to d_model.\n",
    "        device: torch.device where the models reside.\n",
    "        \n",
    "    Returns:\n",
    "        avg_ADE (float): Average Displacement Error averaged over all samples.\n",
    "        avg_FDE (float): Final Displacement Error averaged over all samples.\n",
    "    \"\"\"\n",
    "    gnn_model.eval()\n",
    "    transformer_model.eval()\n",
    "    decoder_input_mapping.eval()\n",
    "    \n",
    "    total_ADE = 0.0\n",
    "    total_FDE = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample_index in range(len(dataset)):\n",
    "            # Get one sample from the dataset:\n",
    "            input_frames, tgt_positions = dataset[sample_index]\n",
    "            \n",
    "            # Process input frames with the GNN to obtain the source embeddings.\n",
    "            frame_embeddings = []\n",
    "            for frame in input_frames:\n",
    "                x = frame.x.to(device)\n",
    "                edge_index = frame.edge_index.to(device)\n",
    "                # Create a batch vector for a single graph (all nodes belong to graph 0)\n",
    "                batch_vector = torch.zeros(x.size(0), dtype=torch.long).to(device)\n",
    "                emb = gnn_model(x, edge_index, batch_vector)  # Output shape: [1, d_model]\n",
    "                frame_embeddings.append(emb.squeeze(0))       # [d_model]\n",
    "            \n",
    "            # Stack the embeddings to form the source sequence: [1, input_seq_len, d_model]\n",
    "            src_embeddings = torch.stack(frame_embeddings, dim=0).unsqueeze(0)\n",
    "            \n",
    "            # Prepare decoder input using teacher forcing:\n",
    "            # Convert target positions tensor to shape [1, target_seq_len, 2]\n",
    "            tgt_positions_tensor = tgt_positions.unsqueeze(0).to(device)\n",
    "            # Map ground truth positions (except the last time step) to the decoder embedding space.\n",
    "            decoder_inputs_mapped = decoder_input_mapping(tgt_positions_tensor[:, :-1, :])\n",
    "            # Create a start token (zeros) and prepend it.\n",
    "            start_token = torch.zeros(1, 1, src_embeddings.size(-1), device=device)\n",
    "            decoder_inputs = torch.cat([start_token, decoder_inputs_mapped], dim=1)\n",
    "            \n",
    "            # Get the predicted trajectory from the Transformer.\n",
    "            pred_positions = transformer_model(src_embeddings, decoder_inputs)  # [1, target_seq_len, 2]\n",
    "            pred_positions = pred_positions.squeeze(0).cpu().numpy()  # [target_seq_len, 2]\n",
    "            true_positions = tgt_positions.cpu().numpy()              # [target_seq_len, 2]\n",
    "            \n",
    "            # Compute Euclidean distances per timestep:\n",
    "            errors = np.linalg.norm(pred_positions - true_positions, axis=1)\n",
    "            ADE = errors.mean()\n",
    "            FDE = errors[-1]\n",
    "            \n",
    "            total_ADE += ADE\n",
    "            total_FDE += FDE\n",
    "            count += 1\n",
    "    \n",
    "    avg_ADE = total_ADE / count\n",
    "    avg_FDE = total_FDE / count\n",
    "    return avg_ADE, avg_FDE\n",
    "\n",
    "# Evaluate the model on your dataset:\n",
    "avg_ADE, avg_FDE = evaluate_model(dataset, gnn_model, transformer_model, decoder_input_mapping, device)\n",
    "print(f\"Average ADE: {avg_ADE:.4f}\")\n",
    "print(f\"Average FDE: {avg_FDE:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning (RL) for Dynamic Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2A. Custom Gym Environment with Trajectory Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "# from gym import spaces\n",
    "# import pybullet as p\n",
    "# import pybullet_data\n",
    "# import time\n",
    "# import numpy as np\n",
    "\n",
    "# class IntegratedDynamicBehaviorEnv(gym.Env):\n",
    "#     \"\"\"\n",
    "#     A custom environment that simulates an agent in a dynamic scenario using PyBullet.\n",
    "#     The observation is augmented with predicted future trajectories from the GNN+Transformer.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, gnn_model, transformer_model, decoder_input_mapping, \n",
    "#                  dataset_history, input_seq_len, target_seq_len, render_mode=False):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             gnn_model, transformer_model, decoder_input_mapping: Your trained prediction models.\n",
    "#             dataset_history: A list of recent frames (graphs) to form the input sequence.\n",
    "#             input_seq_len (int): Number of past frames used as input.\n",
    "#             target_seq_len (int): Number of future frames to predict.\n",
    "#             render_mode (bool): Whether to render the simulation.\n",
    "#         \"\"\"\n",
    "#         super(IntegratedDynamicBehaviorEnv, self).__init__()\n",
    "#         self.gnn_model = gnn_model\n",
    "#         self.transformer_model = transformer_model\n",
    "#         self.decoder_input_mapping = decoder_input_mapping\n",
    "#         self.input_seq_len = input_seq_len\n",
    "#         self.target_seq_len = target_seq_len\n",
    "#         self.render_mode = render_mode\n",
    "        \n",
    "#         # Define action space, for example, continuous actions for linear and angular velocity.\n",
    "#         self.action_space = spaces.Box(low=np.array([-1, -1]), high=np.array([1, 1]), dtype=np.float32)\n",
    "#         # Define observation space: you can start with agent state (position, orientation)\n",
    "#         # plus predicted trajectory (flattened). For example, agent (2 dims) + goal (2 dims) + predicted trajectory (target_seq_len*2).\n",
    "#         obs_dim = 2 + 2 + target_seq_len * 2\n",
    "#         self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)\n",
    "        \n",
    "#         # PyBullet setup.\n",
    "#         if self.render_mode:\n",
    "#             p.connect(p.GUI)\n",
    "#         else:\n",
    "#             p.connect(p.DIRECT)\n",
    "#         p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "#         self._setup_simulation()\n",
    "        \n",
    "#         # For simplicity, initialize history with a fixed list (in practice, update this over time).\n",
    "#         self.history = dataset_history  # Assume this is a list of recent graph frames (length >= input_seq_len)\n",
    "    \n",
    "#     def _setup_simulation(self):\n",
    "#         p.resetSimulation()\n",
    "#         p.setGravity(0, 0, -9.8)\n",
    "#         self.plane = p.loadURDF(\"plane.urdf\")\n",
    "#         # Load a simple agent (using, e.g., r2d2.urdf)\n",
    "#         start_pos = [0, 0, 0.1]\n",
    "#         start_orientation = p.getQuaternionFromEuler([0, 0, 0])\n",
    "#         self.agent = p.loadURDF(\"r2d2.urdf\", start_pos, start_orientation)\n",
    "#         # Define a random goal position.\n",
    "#         self.goal = np.array([np.random.uniform(-5, 5), np.random.uniform(-5, 5)])\n",
    "#         self.current_step = 0\n",
    "\n",
    "#     def _get_agent_state(self):\n",
    "#         pos, _ = p.getBasePositionAndOrientation(self.agent)\n",
    "#         pos = np.array(pos)[:2]\n",
    "#         return pos\n",
    "\n",
    "#     def _predict_trajectory(self):\n",
    "#         \"\"\"\n",
    "#         Use the last input_seq_len frames from self.history to predict future positions.\n",
    "#         \"\"\"\n",
    "#         if len(self.history) < self.input_seq_len:\n",
    "#             # Not enough history: return zeros.\n",
    "#             return np.zeros((self.target_seq_len, 2))\n",
    "        \n",
    "#         input_frames = self.history[-self.input_seq_len:]\n",
    "#         sample_embeddings = []\n",
    "#         for frame in input_frames:\n",
    "#             x = frame.x.to(device)\n",
    "#             edge_index = frame.edge_index.to(device)\n",
    "#             batch_vector = torch.zeros(x.size(0), dtype=torch.long, device=device)\n",
    "#             emb = self.gnn_model(x, edge_index, batch_vector)\n",
    "#             sample_embeddings.append(emb.squeeze(0))\n",
    "#         src_embeddings = torch.stack(sample_embeddings, dim=0).unsqueeze(0)\n",
    "        \n",
    "#         # For decoder input, use ground truth from history if available; here we use zeros as start.\n",
    "#         start_token = torch.zeros(1, 1, src_embeddings.size(-1), device=device)\n",
    "#         # For simplicity, we form a dummy decoder input by mapping zeros.\n",
    "#         dummy_decoder = decoder_input_mapping(torch.zeros(1, self.target_seq_len - 1, 2, device=device))\n",
    "#         decoder_inputs = torch.cat([start_token, dummy_decoder], dim=1)\n",
    "        \n",
    "#         pred = self.transformer_model(src_embeddings, decoder_inputs)\n",
    "#         return pred.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "#     def step(self, action):\n",
    "#         self.current_step += 1\n",
    "        \n",
    "#         # Update the agent in the simulation.\n",
    "#         # Here, apply a simple kinematic update (for illustration).\n",
    "#         pos, orn = p.getBasePositionAndOrientation(self.agent)\n",
    "#         pos = np.array(pos)\n",
    "#         theta = p.getEulerFromQuaternion(orn)[2]\n",
    "#         dt = 0.1\n",
    "#         linear, angular = action\n",
    "#         new_pos = pos.copy()\n",
    "#         new_pos[0] += linear * np.cos(theta) * dt\n",
    "#         new_pos[1] += linear * np.sin(theta) * dt\n",
    "#         new_theta = theta + angular * dt\n",
    "#         new_orn = p.getQuaternionFromEuler([0, 0, new_theta])\n",
    "#         p.resetBasePositionAndOrientation(self.agent, new_pos.tolist(), new_orn)\n",
    "#         p.stepSimulation()\n",
    "#         if self.render_mode:\n",
    "#             time.sleep(0.01)\n",
    "        \n",
    "#         # Get current agent state.\n",
    "#         agent_state = self._get_agent_state()\n",
    "        \n",
    "#         # Predict future trajectory using the GNN+Transformer predictor.\n",
    "#         pred_traj = self._predict_trajectory()  # Shape: [target_seq_len, 2]\n",
    "        \n",
    "#         # Form observation: [agent_state (2 dims), goal (2 dims), pred_traj flattened (target_seq_len*2)]\n",
    "#         obs = np.concatenate([agent_state, self.goal, pred_traj.flatten()])\n",
    "        \n",
    "#         # Compute reward, e.g., negative distance to goal.\n",
    "#         reward = -np.linalg.norm(agent_state - self.goal)\n",
    "#         done = self.current_step >= 100 or np.linalg.norm(agent_state - self.goal) < 0.5\n",
    "        \n",
    "#         # (Optional) Update history with a new graph representing the current frame.\n",
    "#         # For simplicity, we reuse an existing frame from the dataset.\n",
    "#         # In practice, you might create a new graph from sensor data.\n",
    "#         self.history.append(self.history[-1])  # dummy update\n",
    "        \n",
    "#         return obs, reward, done, {}\n",
    "\n",
    "#     def reset(self):\n",
    "#         self._setup_simulation()\n",
    "#         self.current_step = 0\n",
    "#         # Reset history: for simplicity, use the first input_seq_len frames from the dataset.\n",
    "#         self.history = dataset.graph_list[:self.input_seq_len]\n",
    "#         pos = self._get_agent_state()\n",
    "#         pred_traj = self._predict_trajectory()\n",
    "#         obs = np.concatenate([pos, self.goal, pred_traj.flatten()])\n",
    "#         return obs\n",
    "\n",
    "#     def render(self, mode='human'):\n",
    "#         pass\n",
    "\n",
    "#     def close(self):\n",
    "#         p.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2B. Train an RL Agent with PPO on the Integrated Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 30 2025 14:18:15\n"
     ]
    }
   ],
   "source": [
    "# import gymnasium as gym\n",
    "# from gymnasium import spaces\n",
    "# import pybullet as p\n",
    "# import pybullet_data\n",
    "# import time\n",
    "import numpy as np\n",
    "# import random\n",
    "import cv2\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.data import Data\n",
    "# from torch_geometric.nn import GATConv, global_mean_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----- Load Additional Files -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read destinations from 'destinations.txt'. Each line contains destination coordinates.\n",
    "destinations = []\n",
    "with open(\"ewap_dataset/seq_eth/destinations.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) >= 2:\n",
    "            destinations.append([float(parts[0]), float(parts[1])])\n",
    "# You can similarly load groups from 'groups.txt' if you wish to incorporate group dynamics.\n",
    "# And obstacles from 'map.png' and H.txt can be loaded and transformed if desired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-20.0, 5.8566027],\n",
       " [-6.5902743, 0.065724367],\n",
       " [-6.5553084, 11.867515],\n",
       " [15.107171, 5.5659299]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load groups.txt (each line: a list of pedestrian IDs forming a group) (NEVER USED)\n",
    "def load_groups(groups_file):\n",
    "    groups = []\n",
    "    with open(groups_file, 'r') as f:\n",
    "        for line in f:\n",
    "            # Split by whitespace and convert to integers\n",
    "            group = list(map(int, line.split()))\n",
    "            groups.append(group)\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "1. Load obstacles from map.png using the homography matrix (H.txt)\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obstacles_from_map(map_file, H_file, min_area=50):\n",
    "    map_img = cv2.imread(map_file, cv2.IMREAD_GRAYSCALE)\n",
    "    if map_img is None:\n",
    "        raise ValueError(\"Map image not found!\")\n",
    "    _, thresh = cv2.threshold(map_img, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "    cv2.imshow(\"Thresholded\", thresh)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_img = np.zeros_like(map_img)\n",
    "    cv2.drawContours(contour_img, contours, -1, (255, 255, 255), 2)\n",
    "    cv2.imshow(\"Contours\", contour_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    obstacle_positions = []\n",
    "    obstacle_sizes = []\n",
    "    H = np.loadtxt(H_file)\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area < min_area:\n",
    "            continue\n",
    "        M = cv2.moments(cnt)\n",
    "        if M[\"m00\"] == 0:\n",
    "            continue\n",
    "        cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "        img_point = np.array([cx, cy, 1])\n",
    "        world_point = H @ img_point\n",
    "        world_point = world_point / world_point[2]\n",
    "        obstacle_positions.append(world_point[:2])\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        obstacle_sizes.append([w * 0.01, h * 0.01])\n",
    "    return np.array(obstacle_positions), np.array(obstacle_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map bounds (world coordinates): [-10.09475670896092, 19.63625293233163, -10.941189442074746, 13.872841783436563]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def compute_map_bounds(map_file, H_file):\n",
    "    \"\"\"\n",
    "    Compute map boundaries (xmin, xmax, ymin, ymax) in world coordinates using the corners\n",
    "    of the map image and the homography matrix in H.txt.\n",
    "    \n",
    "    Args:\n",
    "        map_file (str): Path to the map image (map.png).\n",
    "        H_file (str): Path to the homography matrix file (H.txt).\n",
    "    \n",
    "    Returns:\n",
    "        A list: [xmin, xmax, ymin, ymax]\n",
    "    \"\"\"\n",
    "    # Load the map image\n",
    "    map_img = cv2.imread(map_file, cv2.IMREAD_GRAYSCALE)\n",
    "    if map_img is None:\n",
    "        raise ValueError(\"Map image not found!\")\n",
    "    h, w = map_img.shape  # height, width of image\n",
    "    \n",
    "    # Define the four corners of the image in homogeneous coordinates.\n",
    "    corners = np.array([[0, 0, 1],\n",
    "                        [w, 0, 1],\n",
    "                        [0, h, 1],\n",
    "                        [w, h, 1]]).T  # shape: (3, 4)\n",
    "    \n",
    "    # Load the homography matrix.\n",
    "    H = np.loadtxt(H_file)  # shape: (3,3)\n",
    "    \n",
    "    # Transform the corners to world coordinates.\n",
    "    world_corners = H @ corners  # shape: (3,4)\n",
    "    world_corners = world_corners / world_corners[2, :]  # normalize\n",
    "    world_corners = world_corners[:2, :]  # take x and y coordinates, shape: (2,4)\n",
    "    \n",
    "    xmin = float(np.min(world_corners[0, :]))\n",
    "    xmax = float(np.max(world_corners[0, :]))\n",
    "    ymin = float(np.min(world_corners[1, :]))\n",
    "    ymax = float(np.max(world_corners[1, :]))\n",
    "    \n",
    "    return [xmin, xmax, ymin, ymax]\n",
    "\n",
    "# Example usage:\n",
    "map_file = \"ewap_dataset/seq_eth/map.png\"\n",
    "H_file = \"ewap_dataset/seq_eth/H.txt\"\n",
    "map_bounds = compute_map_bounds(map_file, H_file)\n",
    "print(\"Map bounds (world coordinates):\", map_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x56d28b30) is not the object's thread (0x56d61e00).\n",
      "Cannot move to target thread (0x56d28b30)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# File paths (adjust as needed)\n",
    "map_file = \"ewap_dataset/seq_eth/map.png\"\n",
    "H_file = \"ewap_dataset/seq_eth/H.txt\"\n",
    "obs_positions, obs_sizes = load_obstacles_from_map(map_file, H_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.99466341, 1.96317765]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "2. Integrated RL Environment (Gymnasium Version)\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "# import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "# from torch_geometric.nn import GATConv, global_mean_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "\n",
    "# Assume that these normalization parameters are already computed:\n",
    "# For example:\n",
    "# mean = obsmat_df[['pos_x', 'pos_y', 'v_x', 'v_y']].mean()\n",
    "# std = obsmat_df[['pos_x', 'pos_y', 'v_x', 'v_y']].std()\n",
    "# They should be accessible here as global variables.\n",
    "# Also assume `device` is defined (e.g., device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "class IntegratedDynamicBehaviorEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom Gymnasium environment that simulates an agent in a dynamic scenario using PyBullet.\n",
    "    The observation is augmented with predicted future trajectories from the GNN+Transformer.\n",
    "    The destination (final goal) is fixed for the episode.\n",
    "    Obstacles are loaded from a map image using the homography matrix, and the map boundaries are computed\n",
    "    from the image so that the agent is penalized if it goes outside.\n",
    "    \"\"\"\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "    \n",
    "    def __init__(self, gnn_model, transformer_model, decoder_input_mapping, \n",
    "                 dataset_history, input_seq_len, target_seq_len, destinations,\n",
    "                 obs_positions, obs_sizes, map_bounds, render_mode=False):\n",
    "        super(IntegratedDynamicBehaviorEnv, self).__init__()\n",
    "        self.gnn_model = gnn_model\n",
    "        self.transformer_model = transformer_model\n",
    "        self.decoder_input_mapping = decoder_input_mapping\n",
    "        self.input_seq_len = input_seq_len\n",
    "        self.target_seq_len = target_seq_len\n",
    "        self.render_mode = render_mode\n",
    "        \n",
    "        self.destinations = destinations  # final goals from destinations.txt (in real-world coords)\n",
    "        self.obs_positions = obs_positions  # obstacles positions (real-world)\n",
    "        self.obs_sizes = obs_sizes          # obstacles sizes\n",
    "        self.map_bounds = map_bounds        # computed from map.png and H.txt (real-world)\n",
    "        \n",
    "        # Save initial dataset history (these should be normalized graphs)\n",
    "        self.dataset_history = dataset_history\n",
    "        \n",
    "        # Define action space: continuous [linear_velocity, angular_velocity].\n",
    "        self.action_space = spaces.Box(low=np.array([-1, -1]), high=np.array([1, 1]), dtype=np.float32)\n",
    "        # Define observation space: agent position (2), goal (2), predicted trajectory (target_seq_len * 2).\n",
    "        obs_dim = 2 + 2 + target_seq_len * 2\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)\n",
    "        \n",
    "        if self.render_mode:\n",
    "            p.connect(p.GUI)\n",
    "        else:\n",
    "            p.connect(p.DIRECT)\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        self._setup_simulation()\n",
    "        self.history = list(self.dataset_history)  # Copy initial history (normalized)\n",
    "        self.current_step = 0\n",
    "\n",
    "    def _setup_simulation(self):\n",
    "        p.resetSimulation()\n",
    "        p.setGravity(0, 0, -9.8)\n",
    "        self.plane = p.loadURDF(\"plane.urdf\")\n",
    "        # Load obstacles into simulation.\n",
    "        self.obstacle_ids = []\n",
    "        for pos, size in zip(self.obs_positions, self.obs_sizes):\n",
    "            obstacle_uid = p.loadURDF(\"cube_small.urdf\", [float(pos[0]), float(pos[1]), 0.1])\n",
    "            self.obstacle_ids.append(obstacle_uid)\n",
    "        # Load the agent.\n",
    "        start_pos = [0, 0, 0.1]\n",
    "        start_orientation = p.getQuaternionFromEuler([0, 0, 0])\n",
    "        self.agent = p.loadURDF(\"r2d2.urdf\", start_pos, start_orientation)\n",
    "        # Set the final destination (goal) for the episode.\n",
    "        self.goal = random.choice(self.destinations)\n",
    "        self.current_step = 0\n",
    "\n",
    "    def _get_agent_state(self):\n",
    "        pos, _ = p.getBasePositionAndOrientation(self.agent)\n",
    "        return np.array(pos)[:2]  # Real-world coordinates\n",
    "\n",
    "    def _create_graph_from_state(self, state):\n",
    "        \"\"\"\n",
    "        Convert the agent's state (real-world) to normalized coordinates and create a graph.\n",
    "        \"\"\"\n",
    "        # Normalize the state using global mean and std (for pos_x and pos_y)\n",
    "        norm_x = (state[0] - mean['pos_x']) / std['pos_x']\n",
    "        norm_y = (state[1] - mean['pos_y']) / std['pos_y']\n",
    "        feature = np.array([norm_x, norm_y, 0.0, 0.0])\n",
    "        x = torch.tensor([feature], dtype=torch.float)\n",
    "        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "        return Data(x=x, edge_index=edge_index)\n",
    "    \n",
    "    def _predict_trajectory(self):\n",
    "        if len(self.history) < self.input_seq_len:\n",
    "            return np.zeros((self.target_seq_len, 2))\n",
    "        input_frames = self.history[-self.input_seq_len:]\n",
    "        sample_embeddings = []\n",
    "        for frame in input_frames:\n",
    "            x = frame.x.to(device)\n",
    "            edge_index = frame.edge_index.to(device)\n",
    "            # Create a dummy batch vector for the GNN (if required by your model)\n",
    "            batch_vector = torch.zeros(x.size(0), dtype=torch.long, device=device)\n",
    "            emb = self.gnn_model(x, edge_index, batch_vector)\n",
    "            sample_embeddings.append(emb.squeeze(0))\n",
    "        src_embeddings = torch.stack(sample_embeddings, dim=0).unsqueeze(0)\n",
    "        start_token = torch.zeros(1, 1, src_embeddings.size(-1), device=device)\n",
    "        dummy_decoder = self.decoder_input_mapping(torch.zeros(1, self.target_seq_len - 1, 2, device=device))\n",
    "        decoder_inputs = torch.cat([start_token, dummy_decoder], dim=1)\n",
    "        pred_norm = self.transformer_model(src_embeddings, decoder_inputs)\n",
    "        \n",
    "        # De-normalize predictions:\n",
    "        # Using the mean and std for pos_x and pos_y (global normalization parameters)\n",
    "        pos_mean = torch.tensor([mean['pos_x'], mean['pos_y']], device=device, dtype=torch.float)\n",
    "        pos_std = torch.tensor([std['pos_x'], std['pos_y']], device=device, dtype=torch.float)\n",
    "        pred_real = pred_norm * pos_std + pos_mean\n",
    "        return pred_real.squeeze(0).detach().cpu().numpy()\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        # Get the agent's current real-world state\n",
    "        agent_state = self._get_agent_state()\n",
    "        # Get predicted trajectory in real-world coordinates (de-normalized)\n",
    "        pred_traj = self._predict_trajectory()\n",
    "        # Return observation combining the agent state, goal, and predicted trajectory\n",
    "        return np.concatenate([agent_state, self.goal, pred_traj.flatten()])\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        pos, orn = p.getBasePositionAndOrientation(self.agent)\n",
    "        pos = np.array(pos)\n",
    "        theta = p.getEulerFromQuaternion(orn)[2]\n",
    "        dt = 0.1\n",
    "        linear, angular = action\n",
    "        new_pos = pos.copy()\n",
    "        new_pos[0] += linear * np.cos(theta) * dt\n",
    "        new_pos[1] += linear * np.sin(theta) * dt\n",
    "        new_theta = theta + angular * dt\n",
    "        new_orn = p.getQuaternionFromEuler([0, 0, new_theta])\n",
    "        p.resetBasePositionAndOrientation(self.agent, new_pos.tolist(), new_orn)\n",
    "        p.stepSimulation()\n",
    "        if self.render_mode:\n",
    "            time.sleep(0.01)\n",
    "        obs = self._get_observation()\n",
    "        agent_state = self._get_agent_state()\n",
    "        dist = np.linalg.norm(agent_state - self.goal)\n",
    "        reward = -dist\n",
    "        # Bonus for reaching the goal.\n",
    "        if dist < 0.5:\n",
    "            reward += 100\n",
    "        # If agent goes out-of-bounds, add a heavy penalty.\n",
    "        xmin, xmax, ymin, ymax = self.map_bounds\n",
    "        if not (xmin <= agent_state[0] <= xmax and ymin <= agent_state[1] <= ymax):\n",
    "            reward -= 100\n",
    "        terminated = (self.current_step >= 100 or dist < 0.5)\n",
    "        truncated = False\n",
    "        # IMPORTANT: When adding the new state to history, normalize it so that the model always sees normalized inputs.\n",
    "        new_graph = self._create_graph_from_state(self._get_agent_state())\n",
    "        self.history.append(new_graph)\n",
    "        if len(self.history) > self.input_seq_len:\n",
    "            self.history.pop(0)\n",
    "        return obs, reward, terminated, truncated, {}\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        self._setup_simulation()\n",
    "        self.current_step = 0\n",
    "        # Reset history using the normalized dataset history provided at initialization\n",
    "        self.history = list(self.dataset_history[:self.input_seq_len])\n",
    "        obs = self._get_observation()\n",
    "        return obs, {}\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        p.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "PPO Training using Stable-Baselines3 (Gymnasium Version)\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 22:35:05.815464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741280705.963164    1509 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741280705.995830    1509 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-06 22:35:06.264272: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch as th\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "# from stable_baselines3.common.type_aliases import Schedule\n",
    "# import gym\n",
    "\n",
    "# # Custom MLP extractor: builds separate networks for actor and critic.\n",
    "# class CustomMlpExtractor(nn.Module):\n",
    "#     def __init__(self, feature_dim: int, net_arch_actor=[128, 128], net_arch_critic=[128, 128]):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             feature_dim: Dimension of features from the feature extractor.\n",
    "#             net_arch_actor: List of hidden layer sizes for the actor network.\n",
    "#             net_arch_critic: List of hidden layer sizes for the critic network.\n",
    "#         \"\"\"\n",
    "#         super(CustomMlpExtractor, self).__init__()\n",
    "#         # Build actor network.\n",
    "#         actor_layers = []\n",
    "#         input_dim = feature_dim\n",
    "#         for layer_size in net_arch_actor:\n",
    "#             actor_layers.append(nn.Linear(input_dim, layer_size))\n",
    "#             actor_layers.append(nn.ReLU())\n",
    "#             input_dim = layer_size\n",
    "#         self.actor_net = nn.Sequential(*actor_layers)\n",
    "#         self.actor_out_dim = input_dim\n",
    "\n",
    "#         # Build critic network.\n",
    "#         critic_layers = []\n",
    "#         input_dim = feature_dim\n",
    "#         for layer_size in net_arch_critic:\n",
    "#             critic_layers.append(nn.Linear(input_dim, layer_size))\n",
    "#             critic_layers.append(nn.ReLU())\n",
    "#             input_dim = layer_size\n",
    "#         self.critic_net = nn.Sequential(*critic_layers)\n",
    "#         self.critic_out_dim = input_dim\n",
    "\n",
    "#     def forward(self, features: th.Tensor):\n",
    "#         # Returns separate latent features for actor and critic.\n",
    "#         actor_features = self.actor_net(features)\n",
    "#         critic_features = self.critic_net(features)\n",
    "#         return actor_features, critic_features\n",
    "\n",
    "# # Custom Policy that uses the custom MLP extractor.\n",
    "# class CustomPolicy(ActorCriticPolicy):\n",
    "#     \"\"\"\n",
    "#     A custom PPO policy with separate actor and critic networks.\n",
    "#     Implements forward_actor and forward_critic methods to satisfy the interface.\n",
    "#     \"\"\"\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         observation_space: gym.spaces.Space,\n",
    "#         action_space: gym.spaces.Space,\n",
    "#         learning_rate: Schedule,\n",
    "#         net_arch: dict = None,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         if net_arch is None:\n",
    "#             net_arch = dict(pi=[128, 128], vf=[128, 128])\n",
    "#         super(CustomPolicy, self).__init__(\n",
    "#             observation_space,\n",
    "#             action_space,\n",
    "#             learning_rate,\n",
    "#             net_arch=net_arch,\n",
    "#             activation_fn=nn.ReLU,\n",
    "#             ortho_init=False,\n",
    "#             **kwargs,\n",
    "#         )\n",
    "#         # Create custom MLP extractor.\n",
    "#         self.mlp_extractor = CustomMlpExtractor(\n",
    "#             self.features_dim,\n",
    "#             net_arch_actor=net_arch[\"pi\"],\n",
    "#             net_arch_critic=net_arch[\"vf\"]\n",
    "#         )\n",
    "#         # Final actor and critic layers.\n",
    "#         self.actor_net = nn.Linear(self.mlp_extractor.actor_out_dim, self.action_space.shape[0])\n",
    "#         self.value_net = nn.Linear(self.mlp_extractor.critic_out_dim, 1)\n",
    "#         self._initialize_weights(self.mlp_extractor)\n",
    "#         self._initialize_weights(self.actor_net)\n",
    "#         self._initialize_weights(self.value_net)\n",
    "\n",
    "#     def _initialize_weights(self, module: nn.Module):\n",
    "#         \"\"\"Initialize weights for Linear layers using orthogonal initialization.\"\"\"\n",
    "#         for m in module.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 nn.init.orthogonal_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#     def _get_latent_pi(self, features: th.Tensor) -> th.Tensor:\n",
    "#         actor_features, _ = self.mlp_extractor(features)\n",
    "#         return actor_features\n",
    "\n",
    "#     def _get_latent_vf(self, features: th.Tensor) -> th.Tensor:\n",
    "#         _, critic_features = self.mlp_extractor(features)\n",
    "#         return critic_features\n",
    "\n",
    "#     def forward(self, obs: th.Tensor, deterministic: bool = False):\n",
    "#         features = self.extract_features(obs)\n",
    "#         latent_pi = self._get_latent_pi(features)\n",
    "#         latent_vf = self._get_latent_vf(features)\n",
    "#         action_mean = self.actor_net(latent_pi)\n",
    "#         value = self.value_net(latent_vf)\n",
    "#         dist = self._get_action_dist_from_mean(action_mean)\n",
    "#         if deterministic:\n",
    "#             actions = dist.mean\n",
    "#         else:\n",
    "#             actions = dist.sample()\n",
    "#         log_prob = dist.log_prob(actions)\n",
    "#         return actions, value, log_prob\n",
    "\n",
    "#     def _get_action_dist_from_mean(self, action_mean: th.Tensor):\n",
    "#         # Diagonal Gaussian distribution with fixed standard deviation.\n",
    "#         action_std = th.ones_like(action_mean) * 0.5\n",
    "#         return th.distributions.Independent(th.distributions.Normal(action_mean, action_std), 1)\n",
    "\n",
    "#     # Provide explicit methods for actor and critic forward passes.\n",
    "#     def forward_actor(self, obs: th.Tensor) -> th.Tensor:\n",
    "#         features = self.extract_features(obs)\n",
    "#         latent_pi = self._get_latent_pi(features)\n",
    "#         return self.actor_net(latent_pi)\n",
    "\n",
    "#     def forward_critic(self, obs: th.Tensor) -> th.Tensor:\n",
    "#         features = self.extract_features(obs)\n",
    "#         latent_vf = self._get_latent_vf(features)\n",
    "#         return self.value_net(latent_vf)\n",
    "\n",
    "#     def _predict(self, obs: th.Tensor, deterministic: bool = False):\n",
    "#         return self.forward(obs, deterministic=deterministic)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[1, 4], edge_index=[2, 1]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[2, 4], edge_index=[2, 2]),\n",
       " Data(x=[1, 4], edge_index=[2, 1])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_history = dataset.graph_list[:input_seq_len]\n",
    "initial_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 100       |\n",
      "|    ep_rew_mean     | -1.55e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 19        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 106       |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | -1.46e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 218         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003952888 |\n",
      "|    clip_fraction        | 0.015       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | -0.000412   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.36e+04    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00265    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 5.45e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | -1.43e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 328          |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023371181 |\n",
      "|    clip_fraction        | 0.00142      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.000102     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 3.03e+04     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000933    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.53e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | -1.47e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 436          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059714667 |\n",
      "|    clip_fraction        | 0.0326       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -0.000161    |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 2.23e+04     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00419     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.38e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | -1.46e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 546         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003524505 |\n",
      "|    clip_fraction        | 0.00444     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -2.86e-06   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.42e+04    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00137    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 5.74e+04    |\n",
      "-----------------------------------------\n",
      "RL agent training complete and saved.\n"
     ]
    }
   ],
   "source": [
    "initial_history = dataset.graph_list[:input_seq_len]\n",
    "\n",
    "env = IntegratedDynamicBehaviorEnv(gnn_model, transformer_model, decoder_input_mapping,\n",
    "                                    dataset_history=initial_history,\n",
    "                                    input_seq_len=input_seq_len,\n",
    "                                    target_seq_len=target_seq_len,\n",
    "                                    destinations=destinations,\n",
    "                                    obs_positions=obs_positions,\n",
    "                                    obs_sizes=obs_sizes,\n",
    "                                    map_bounds=map_bounds,\n",
    "                                    render_mode=False)\n",
    "env = Monitor(env)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "rl_model = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=1e-4)\n",
    "rl_model.learn(total_timesteps=10000)\n",
    "rl_model.save(\"ppo_dynamic_behavior_agent\")\n",
    "print(\"RL agent training complete and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Function to convert world (simulation) coordinates to video pixel coordinates.\n",
    "def world_to_pixel(world_pos, map_bounds, frame_width, frame_height):\n",
    "    \"\"\"\n",
    "    Convert world coordinates (x, y) into pixel coordinates.\n",
    "    map_bounds: [xmin, xmax, ymin, ymax]\n",
    "    \"\"\"\n",
    "    xmin, xmax, ymin, ymax = map_bounds\n",
    "    x_world, y_world = world_pos\n",
    "    # Map x from [xmin, xmax] to [0, frame_width]\n",
    "    x_pixel = int((x_world - xmin) / (xmax - xmin) * frame_width)\n",
    "    # Map y from [ymin, ymax] to [frame_height, 0] (inverting the y-axis)\n",
    "    y_pixel = int(frame_height - ((y_world - ymin) / (ymax - ymin) * frame_height))\n",
    "    return (x_pixel, y_pixel)\n",
    "\n",
    "# ---------------------------\n",
    "# Run the trained RL agent in simulation and record positions.\n",
    "# ---------------------------\n",
    "\n",
    "positions = []  # List to store agent's (x,y) positions.\n",
    "\n",
    "# Reset the environment; if using DummyVecEnv, obs is a 2D array.\n",
    "result = env.reset()\n",
    "if isinstance(result, tuple):\n",
    "    obs, _ = result\n",
    "else:\n",
    "    obs = result\n",
    "\n",
    "# If obs is a batch (2D array), extract the first observation.\n",
    "if obs.ndim == 2:\n",
    "    current_obs = obs[0]\n",
    "else:\n",
    "    current_obs = obs\n",
    "\n",
    "max_steps = 100  # Episode length\n",
    "\n",
    "for step in range(max_steps):\n",
    "    # Get action from your PPO agent.\n",
    "    action, _ = rl_model.predict(current_obs)\n",
    "    # Step the environment.\n",
    "    result = env.step(action)\n",
    "    # Depending on your environment, result can have 5 values (Gymnasium style) or 4.\n",
    "    if len(result) == 5:\n",
    "        obs, reward, terminated, truncated, info = result\n",
    "        done = terminated or truncated\n",
    "    else:\n",
    "        obs, reward, done, info = result\n",
    "    # If obs is a batch, take the first one.\n",
    "    if obs.ndim == 2:\n",
    "        current_obs = obs[0]\n",
    "    else:\n",
    "        current_obs = obs\n",
    "    # Our observation is: [agent_state (2), goal (2), predicted trajectory ...]\n",
    "    # The first two numbers represent the agent's position.\n",
    "    agent_pos = current_obs[:2]\n",
    "    positions.append(agent_pos)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# ---------------------------\n",
    "# Open the source video (\"deq_eth.avi\") and prepare to write output video.\n",
    "# ---------------------------\n",
    "source_video = \"ewap_dataset/seq_eth/seq_eth.avi\"\n",
    "output_video = \"output_agent.avi\"\n",
    "\n",
    "cap = cv2.VideoCapture(source_video)\n",
    "if not cap.isOpened():\n",
    "    raise Exception(\"Error opening source video file\")\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Set up the video writer.\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(output_video, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# ---------------------------\n",
    "# Overlay the recorded positions (agent's path) on the video.\n",
    "# ---------------------------\n",
    "frame_idx = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # If we have a recorded position for this frame, map it.\n",
    "    if frame_idx < len(positions):\n",
    "        pos_world = positions[frame_idx]\n",
    "        pos_pixel = world_to_pixel(pos_world, map_bounds, frame_width, frame_height)\n",
    "        # Draw a red circle at the agent's position.\n",
    "        cv2.circle(frame, pos_pixel, radius=5, color=(0, 0, 255), thickness=-1)\n",
    "    # Optionally, draw the entire path up to this frame.\n",
    "    for i in range(1, min(frame_idx, len(positions))):\n",
    "        pt1 = world_to_pixel(positions[i-1], map_bounds, frame_width, frame_height)\n",
    "        pt2 = world_to_pixel(positions[i], map_bounds, frame_width, frame_height)\n",
    "        cv2.line(frame, pt1, pt2, color=(255, 0, 0), thickness=2)\n",
    "    out.write(frame)\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Output video saved as\", output_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_history = dataset.graph_list[:input_seq_len]\n",
    "\n",
    "env = IntegratedDynamicBehaviorEnv(gnn_model, transformer_model, decoder_input_mapping,\n",
    "                                    dataset_history=initial_history,\n",
    "                                    input_seq_len=input_seq_len,\n",
    "                                    target_seq_len=target_seq_len,\n",
    "                                    destinations=destinations,\n",
    "                                    obs_positions=obs_positions,\n",
    "                                    obs_sizes=obs_sizes,\n",
    "                                    map_bounds=map_bounds,\n",
    "                                    render_mode=False)\n",
    "env = Monitor(env)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "lr_schedule = lambda _: 1e-4  # Constant learning rate schedule.\n",
    "\n",
    "# Initialize PPO with the custom policy.\n",
    "rl_model = PPO(CustomPolicy, env, verbose=1, learning_rate=lr_schedule, tensorboard_log=\"./logs/\")\n",
    "\n",
    "# Train the PPO agent.\n",
    "rl_model.learn(total_timesteps=10000)\n",
    "rl_model.save(\"ppo_custom_policy_agent\")\n",
    "print(\"RL agent training complete and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create the integrated environment.\n",
    "# For the environment's history, we pass in an initial list of graphs.\n",
    "initial_history = dataset.graph_list[:input_seq_len]  # use the first few frames as history\n",
    "\n",
    "env = IntegratedDynamicBehaviorEnv(gnn_model, transformer_model, decoder_input_mapping,\n",
    "                                   dataset_history=initial_history,\n",
    "                                   input_seq_len=input_seq_len,\n",
    "                                   target_seq_len=target_seq_len,\n",
    "                                   render_mode=False)\n",
    "\n",
    "# Initialize PPO with a standard MLP policy.\n",
    "rl_model = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=1e-4)\n",
    "\n",
    "# Train the RL agent (adjust timesteps as needed).\n",
    "rl_model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the RL agent.\n",
    "rl_model.save(\"ppo_dynamic_behavior_agent\")\n",
    "print(\"RL agent training complete and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 30 2025 14:18:15\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def predict_future_trajectory(agent_history, gnn_model, transformer_model, decoder_input_mapping,\n",
    "                              device, input_seq_len, target_seq_len):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        agent_history (list): List of past states. Each state is a dict with keys:\n",
    "                              'pos': numpy array of shape (2,)\n",
    "                              'vel': numpy array of shape (2,)\n",
    "                              (Length should be at least input_seq_len.)\n",
    "        gnn_model: Pre-trained GNN model.\n",
    "        transformer_model: Pre-trained Transformer model.\n",
    "        decoder_input_mapping: Linear layer mapping (for decoder inputs).\n",
    "        device: torch.device.\n",
    "        input_seq_len (int): Number of past frames used as input.\n",
    "        target_seq_len (int): Number of future time steps to predict.\n",
    "    \n",
    "    Returns:\n",
    "        pred_positions (numpy array): Predicted future positions of shape [target_seq_len, 2].\n",
    "    \"\"\"\n",
    "    # Build a list of graphs from the last input_seq_len states.\n",
    "    graphs = []\n",
    "    for state in agent_history[-input_seq_len:]:\n",
    "        pos = state['pos']       # shape: (2,)\n",
    "        vel = state['vel']       # shape: (2,)\n",
    "        features = np.concatenate([pos, vel])  # shape: (4,)\n",
    "        features = torch.tensor(features, dtype=torch.float).unsqueeze(0)  # [1, 4]\n",
    "        # With one node, use a self-loop edge.\n",
    "        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "        graph = Data(x=features, edge_index=edge_index)\n",
    "        graphs.append(graph)\n",
    "    \n",
    "    # Process each graph with the GNN.\n",
    "    src_embeddings_list = []\n",
    "    for graph in graphs:\n",
    "        x = graph.x.to(device)             # [1, 4]\n",
    "        edge_index = graph.edge_index.to(device)\n",
    "        node_embedding = gnn_model(x, edge_index)  # [1, d_model]\n",
    "        # For a single node, pooling is trivial:\n",
    "        frame_embedding = node_embedding.mean(dim=0)  # [d_model]\n",
    "        src_embeddings_list.append(frame_embedding)\n",
    "    src_embeddings = torch.stack(src_embeddings_list, dim=0)  # [input_seq_len, d_model]\n",
    "    src_embeddings = src_embeddings.unsqueeze(0)  # [1, input_seq_len, d_model]\n",
    "    \n",
    "    # Prepare a decoder input: use a start token followed by zeros.\n",
    "    d_model = src_embeddings.size(-1)\n",
    "    start_token = torch.zeros((1, 1, d_model), device=device)\n",
    "    zeros = torch.zeros((1, target_seq_len - 1, d_model), device=device)\n",
    "    decoder_input = torch.cat([start_token, zeros], dim=1)  # [1, target_seq_len, d_model]\n",
    "    \n",
    "    # Get predicted trajectory (positions) from the Transformer.\n",
    "    with torch.no_grad():\n",
    "        pred_positions = transformer_model(src_embeddings, decoder_input)  # [1, target_seq_len, 2]\n",
    "    pred_positions = pred_positions.squeeze(0).cpu().numpy()  # [target_seq_len, 2]\n",
    "    return pred_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "\n",
    "class IntegratedDynamicBehaviorEnv(gym.Env):\n",
    "    def __init__(self, gnn_model, transformer_model, decoder_input_mapping, device,\n",
    "                 input_seq_len=8, target_seq_len=12, render_mode=False):\n",
    "        super(IntegratedDynamicBehaviorEnv, self).__init__()\n",
    "        self.gnn_model = gnn_model\n",
    "        self.transformer_model = transformer_model\n",
    "        self.decoder_input_mapping = decoder_input_mapping\n",
    "        self.device = device\n",
    "        self.input_seq_len = input_seq_len\n",
    "        self.target_seq_len = target_seq_len\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Action space: [linear_velocity, angular_velocity]\n",
    "        self.action_space = spaces.Box(low=np.array([-1, -1]), high=np.array([1, 1]), dtype=np.float32)\n",
    "        # Base observation: [agent_x, agent_y, goal_x, goal_y, dist_obs1, dist_obs2, dist_obs3] = 7 dims.\n",
    "        # We augment it with predicted trajectory: target_seq_len * 2 dimensions.\n",
    "        obs_dim = 7 + target_seq_len * 2\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)\n",
    "\n",
    "        # PyBullet setup.\n",
    "        if self.render_mode:\n",
    "            p.connect(p.GUI)\n",
    "        else:\n",
    "            p.connect(p.DIRECT)\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        self._setup_simulation()\n",
    "        \n",
    "        # Buffer for storing recent agent states.\n",
    "        self.agent_history = []\n",
    "\n",
    "    def _setup_simulation(self):\n",
    "        p.resetSimulation()\n",
    "        p.setGravity(0, 0, -9.8)\n",
    "        self.plane = p.loadURDF(\"plane.urdf\")\n",
    "        self._load_agent()\n",
    "        self._load_obstacles()\n",
    "        self.goal = np.array([np.random.uniform(-5, 5), np.random.uniform(-5, 5)])\n",
    "        self.current_step = 0\n",
    "        self.agent_history = []  # reset history\n",
    "\n",
    "    def _load_agent(self):\n",
    "        start_pos = [0, 0, 0.1]\n",
    "        start_orientation = p.getQuaternionFromEuler([0, 0, 0])\n",
    "        self.agent = p.loadURDF(\"r2d2.urdf\", start_pos, start_orientation)\n",
    "\n",
    "    def _load_obstacles(self):\n",
    "        self.obstacles = []\n",
    "        for i in range(3):\n",
    "            pos = [np.random.uniform(-5, 5), np.random.uniform(-5, 5), 0.1]\n",
    "            obs = p.loadURDF(\"cube_small.urdf\", pos, p.getQuaternionFromEuler([0, 0, 0]))\n",
    "            self.obstacles.append(obs)\n",
    "\n",
    "    def reset(self):\n",
    "        self._setup_simulation()\n",
    "        # Initialize agent history with the starting state.\n",
    "        pos, _ = p.getBasePositionAndOrientation(self.agent)\n",
    "        pos = np.array(pos)[:2]\n",
    "        initial_state = {'pos': pos, 'vel': np.array([0, 0])}\n",
    "        self.agent_history.append(initial_state)\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Get current agent position.\n",
    "        pos, _ = p.getBasePositionAndOrientation(self.agent)\n",
    "        pos = np.array(pos)[:2]\n",
    "        \n",
    "        # Compute distances to obstacles.\n",
    "        dists = []\n",
    "        for obs_id in self.obstacles:\n",
    "            obs_pos, _ = p.getBasePositionAndOrientation(obs_id)\n",
    "            obs_pos = np.array(obs_pos)[:2]\n",
    "            dists.append(np.linalg.norm(pos - obs_pos))\n",
    "        while len(dists) < 3:\n",
    "            dists.append(10.0)\n",
    "        base_obs = np.concatenate([pos, self.goal, np.array(dists)])  # shape: (7,)\n",
    "\n",
    "        # Augment with predicted future trajectory from agent_history.\n",
    "        if len(self.agent_history) >= self.input_seq_len:\n",
    "            pred_traj = predict_future_trajectory(self.agent_history, self.gnn_model,\n",
    "                                                  self.transformer_model, self.decoder_input_mapping,\n",
    "                                                  self.device, self.input_seq_len, self.target_seq_len)\n",
    "        else:\n",
    "            pred_traj = np.zeros((self.target_seq_len, 2))\n",
    "        # Flatten predicted trajectory to a vector.\n",
    "        pred_traj_flat = pred_traj.flatten()  # shape: (target_seq_len * 2,)\n",
    "        full_obs = np.concatenate([base_obs, pred_traj_flat])\n",
    "        return full_obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Update agent state based on the action.\n",
    "        linear, angular = action\n",
    "        pos, orn = p.getBasePositionAndOrientation(self.agent)\n",
    "        pos = np.array(pos)\n",
    "        euler = p.getEulerFromQuaternion(orn)\n",
    "        theta = euler[2]\n",
    "        dt = 0.1\n",
    "        new_pos = pos.copy()\n",
    "        new_pos[0] += linear * np.cos(theta) * dt\n",
    "        new_pos[1] += linear * np.sin(theta) * dt\n",
    "        new_theta = theta + angular * dt\n",
    "        new_orn = p.getQuaternionFromEuler([0, 0, new_theta])\n",
    "        p.resetBasePositionAndOrientation(self.agent, new_pos.tolist(), new_orn)\n",
    "        \n",
    "        p.stepSimulation()\n",
    "        if self.render_mode:\n",
    "            time.sleep(0.01)\n",
    "        \n",
    "        # Reward: negative distance to goal (plus bonus for reaching it).\n",
    "        agent_xy = new_pos[:2]\n",
    "        dist_to_goal = np.linalg.norm(agent_xy - self.goal)\n",
    "        reward = -dist_to_goal\n",
    "        done = False\n",
    "        if dist_to_goal < 0.5:\n",
    "            reward += 100\n",
    "            done = True\n",
    "        if self.current_step >= 100:\n",
    "            done = True\n",
    "\n",
    "        # Update agent history with new state.\n",
    "        if len(self.agent_history) > 0:\n",
    "            last_state = self.agent_history[-1]\n",
    "            last_pos = last_state['pos']\n",
    "            vel = (agent_xy - last_pos) / dt\n",
    "        else:\n",
    "            vel = np.array([0, 0])\n",
    "        new_state = {'pos': agent_xy, 'vel': vel}\n",
    "        self.agent_history.append(new_state)\n",
    "        if len(self.agent_history) > self.input_seq_len:\n",
    "            self.agent_history.pop(0)\n",
    "\n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # PyBullet GUI will update automatically if render_mode is True.\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        p.disconnect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shimmy>=2.0\n",
      "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/subhronil/anaconda3/lib/python3.12/site-packages (from shimmy>=2.0) (1.26.4)\n",
      "Requirement already satisfied: gymnasium>=1.0.0a1 in /home/subhronil/anaconda3/lib/python3.12/site-packages (from shimmy>=2.0) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/subhronil/anaconda3/lib/python3.12/site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/subhronil/anaconda3/lib/python3.12/site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/subhronil/anaconda3/lib/python3.12/site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (0.0.4)\n",
      "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
      "Installing collected packages: shimmy\n",
      "Successfully installed shimmy-2.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install 'shimmy>=2.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subhronil/anaconda3/lib/python3.12/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/subhronil/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/home/subhronil/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -377     |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 145         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006203454 |\n",
      "|    clip_fraction        | 0.0555      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.00136    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.1e+03     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00744    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 2.64e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 99.1         |\n",
      "|    ep_rew_mean          | -376         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 216          |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035848282 |\n",
      "|    clip_fraction        | 0.0186       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | -0.00324     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.23e+03     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 2.57e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 99.3        |\n",
      "|    ep_rew_mean          | -383        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 291         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006887097 |\n",
      "|    clip_fraction        | 0.0569      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | -0.000117   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.33e+03    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00585    |\n",
      "|    std                  | 0.979       |\n",
      "|    value_loss           | 3.16e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 99.5         |\n",
      "|    ep_rew_mean          | -388         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 363          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047711213 |\n",
      "|    clip_fraction        | 0.0407       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | -0.000219    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.33e+03     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00548     |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 2.93e+03     |\n",
      "------------------------------------------\n",
      "Integrated RL agent trained and saved.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assume these hyperparameters and pre-trained modules are defined:\n",
    "input_dim = 4           # [pos_x, pos_y, v_x, v_y]\n",
    "hidden_dim = 64\n",
    "d_model = 64            # Should match GNN output.\n",
    "nhead = 8\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "input_seq_len = 8\n",
    "target_seq_len = 12\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load or define your GNN+Transformer models.\n",
    "# (Here we assume they are already trained or at least initialized.)\n",
    "gnn_model = PedestrianGNN(input_dim, hidden_dim, d_model).to(device)\n",
    "transformer_model = TrajectoryTransformer(d_model=d_model, nhead=nhead,\n",
    "                                          num_layers=num_layers, dropout=dropout).to(device)\n",
    "decoder_input_mapping = torch.nn.Linear(2, d_model).to(device)\n",
    "\n",
    "# (Optional) Load saved weights if available:\n",
    "# checkpoint = torch.load(\"trajectory_model_checkpoint.pth\")\n",
    "# gnn_model.load_state_dict(checkpoint['gnn_state_dict'])\n",
    "# transformer_model.load_state_dict(checkpoint['transformer_state_dict'])\n",
    "# decoder_input_mapping.load_state_dict(checkpoint['decoder_input_mapping_state_dict'])\n",
    "\n",
    "# Create the integrated environment.\n",
    "env = IntegratedDynamicBehaviorEnv(gnn_model, transformer_model, decoder_input_mapping,\n",
    "                                   device, input_seq_len, target_seq_len, render_mode=False)\n",
    "\n",
    "# Train PPO on the environment.\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the trained RL agent.\n",
    "model.save(\"ppo_integrated_dynamic_behavior\")\n",
    "print(\"Integrated RL agent trained and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x24941920) is not the object's thread (0x28d6e070).\n",
      "Cannot move to target thread (0x24941920)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlay video saved as seq_eth_overlay.avi\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# --- Helper: Transformation from simulation to video coordinates ---\n",
    "def simulation_to_video_coords(sim_point, scale=50.0, offset=(320, 240)):\n",
    "    \"\"\"\n",
    "    Converts a 2D simulation coordinate (x, y) into video frame coordinates.\n",
    "    This example uses a simple scaling and offset.\n",
    "    Adjust `scale` and `offset` as needed.\n",
    "    \n",
    "    Args:\n",
    "        sim_point: tuple or array (x, y) in simulation units.\n",
    "        scale: scale factor to convert meters (or simulation units) to pixels.\n",
    "        offset: (x_offset, y_offset) in pixels.\n",
    "    Returns:\n",
    "        (int, int): Video frame coordinates.\n",
    "    \"\"\"\n",
    "    x_sim, y_sim = sim_point\n",
    "    # Example: invert y if necessary (depending on video coordinate conventions)\n",
    "    x_img = int(x_sim * scale + offset[0])\n",
    "    y_img = int(y_sim * scale + offset[1])\n",
    "    return (x_img, y_img)\n",
    "\n",
    "# --- Example: Get agent predicted trajectory from your model ---\n",
    "# (Replace this with your actual function/logic for obtaining predictions.)\n",
    "def get_agent_trajectory():\n",
    "    # For demonstration, we use a dummy trajectory in simulation coordinates.\n",
    "    # Suppose the agent predicts a 12-step future trajectory.\n",
    "    trajectory = []\n",
    "    for i in range(12):\n",
    "        # Example: a straight line moving to the right.\n",
    "        trajectory.append((0.1 * i, 0.0))\n",
    "    return trajectory\n",
    "\n",
    "# --- Open the seq_eth.avi video ---\n",
    "video_input_path = \"ewap_dataset/seq_eth/seq_eth.avi\"       # your source video\n",
    "video_output_path = \"seq_eth_overlay.avi\"  # output video with overlay\n",
    "\n",
    "cap = cv2.VideoCapture(video_input_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open video file\", video_input_path)\n",
    "    exit(1)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(video_output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Optionally, you can update these mapping parameters based on your calibration.\n",
    "scale = 50.0       # scale simulation units to pixels (tune this)\n",
    "offset = (frame_width // 2, frame_height // 2)  # shift simulation origin to center of frame\n",
    "\n",
    "# --- Process the video frame by frame ---\n",
    "frame_idx = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Here you would get the current agent position and/or predicted trajectory.\n",
    "    # For demonstration, we use a dummy trajectory.\n",
    "    sim_trajectory = get_agent_trajectory()  # list of (x, y) in simulation coordinates\n",
    "    \n",
    "    # Convert simulation trajectory points to video coordinates.\n",
    "    video_points = [simulation_to_video_coords(pt, scale=scale, offset=offset) \n",
    "                    for pt in sim_trajectory]\n",
    "\n",
    "    # Overlay the trajectory on the frame:\n",
    "    # Draw circles at each predicted position.\n",
    "    for pt in video_points:\n",
    "        cv2.circle(frame, pt, radius=5, color=(0, 0, 255), thickness=-1)\n",
    "    # Draw lines connecting the trajectory points.\n",
    "    for i in range(len(video_points)-1):\n",
    "        cv2.line(frame, video_points[i], video_points[i+1], color=(0, 255, 0), thickness=2)\n",
    "\n",
    "    # Optionally, display frame index or other info.\n",
    "    cv2.putText(frame, f\"Frame: {frame_idx}\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                1, (255, 255, 255), 2)\n",
    "\n",
    "    # Write the frame with overlay to the output video.\n",
    "    out.write(frame)\n",
    "\n",
    "    # For real-time visualization (optional)\n",
    "    cv2.imshow(\"seq_eth with Agent Overlay\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"Overlay video saved as {video_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import PPO\n",
    "\n",
    "# env = IntegratedDynamicBehaviorEnv(gnn_model, transformer_model, decoder_input_mapping,\n",
    "#                                    device, input_seq_len, target_seq_len, render_mode=True)\n",
    "# model = PPO.load(\"ppo_integrated_dynamic_behavior\", env=env)\n",
    "\n",
    "# obs = env.reset()\n",
    "# for _ in range(100):\n",
    "#     action, _ = model.predict(obs)\n",
    "#     obs, reward, done, _ = env.step(action)\n",
    "#     env.render()\n",
    "#     if done:\n",
    "#         obs = env.reset()\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DynamicBehaviorEnv(gym.Env):\n",
    "#     def __init__(self, render_mode=False):\n",
    "#         super(DynamicBehaviorEnv, self).__init__()\n",
    "        \n",
    "#         self.render_mode = render_mode\n",
    "        \n",
    "#         # Action space: [linear_velocity, angular_velocity] (continuous)\n",
    "#         self.action_space = spaces.Box(low=np.array([-1, -1]), high=np.array([1, 1]), dtype=np.float32)\n",
    "        \n",
    "#         # Observation space: For simplicity, we use a vector including:\n",
    "#         # - Agent position (2)\n",
    "#         # - Goal position (2)\n",
    "#         # - Distance to each obstacle (3 distances, padded if needed)\n",
    "#         # Total length = 2 + 2 + 3 = 7 (you can expand this as needed)\n",
    "#         self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(7,), dtype=np.float32)\n",
    "        \n",
    "#         # PyBullet setup\n",
    "#         if self.render_mode:\n",
    "#             p.connect(p.GUI)\n",
    "#         else:\n",
    "#             p.connect(p.DIRECT)\n",
    "#         p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "#         self._setup_simulation()\n",
    "        \n",
    "#     def _setup_simulation(self):\n",
    "#         p.resetSimulation()\n",
    "#         p.setGravity(0, 0, -9.8)\n",
    "#         self.plane = p.loadURDF(\"plane.urdf\")\n",
    "#         self._load_agent()\n",
    "#         self._load_obstacles()\n",
    "#         self.goal = np.array([np.random.uniform(-5, 5), np.random.uniform(-5, 5)])\n",
    "#         self.current_step = 0\n",
    "        \n",
    "#     def _load_agent(self):\n",
    "#         # For demonstration, we use an example robot URDF (e.g., r2d2)\n",
    "#         start_pos = [0, 0, 0.1]\n",
    "#         start_orientation = p.getQuaternionFromEuler([0, 0, 0])\n",
    "#         self.agent = p.loadURDF(\"r2d2.urdf\", start_pos, start_orientation)\n",
    "        \n",
    "#     def _load_obstacles(self):\n",
    "#         self.obstacles = []\n",
    "#         # Create 3 random obstacles (using a small cube URDF)\n",
    "#         for i in range(3):\n",
    "#             pos = [np.random.uniform(-5, 5), np.random.uniform(-5, 5), 0.1]\n",
    "#             obs = p.loadURDF(\"cube_small.urdf\", pos, p.getQuaternionFromEuler([0, 0, 0]))\n",
    "#             self.obstacles.append(obs)\n",
    "    \n",
    "#     def reset(self):\n",
    "#         self._setup_simulation()\n",
    "#         return self._get_obs()\n",
    "    \n",
    "#     def _get_obs(self):\n",
    "#         # Get agent's current 2D position\n",
    "#         pos, _ = p.getBasePositionAndOrientation(self.agent)\n",
    "#         pos = np.array(pos)[:2]\n",
    "        \n",
    "#         # Compute distances to obstacles (if less than 3, pad with a default large value)\n",
    "#         dists = []\n",
    "#         for obs_id in self.obstacles:\n",
    "#             obs_pos, _ = p.getBasePositionAndOrientation(obs_id)\n",
    "#             obs_pos = np.array(obs_pos)[:2]\n",
    "#             dists.append(np.linalg.norm(pos - obs_pos))\n",
    "#         while len(dists) < 3:\n",
    "#             dists.append(10.0)  # arbitrary high distance\n",
    "        \n",
    "#         # Observation: [agent_x, agent_y, goal_x, goal_y, dist_obs1, dist_obs2, dist_obs3]\n",
    "#         obs = np.concatenate([pos, self.goal, np.array(dists)])\n",
    "#         return obs.astype(np.float32)\n",
    "    \n",
    "#     def step(self, action):\n",
    "#         self.current_step += 1\n",
    "        \n",
    "#         # Unpack action: [linear_velocity, angular_velocity]\n",
    "#         linear, angular = action\n",
    "        \n",
    "#         # Get current agent position and orientation\n",
    "#         pos, orn = p.getBasePositionAndOrientation(self.agent)\n",
    "#         pos = np.array(pos)\n",
    "#         euler = p.getEulerFromQuaternion(orn)\n",
    "#         theta = euler[2]\n",
    "        \n",
    "#         # Update agent position (simple kinematic update)\n",
    "#         dt = 0.1\n",
    "#         new_pos = pos.copy()\n",
    "#         new_pos[0] += linear * np.cos(theta) * dt\n",
    "#         new_pos[1] += linear * np.sin(theta) * dt\n",
    "#         new_theta = theta + angular * dt\n",
    "        \n",
    "#         # Apply the new position and orientation\n",
    "#         new_orn = p.getQuaternionFromEuler([0, 0, new_theta])\n",
    "#         p.resetBasePositionAndOrientation(self.agent, new_pos.tolist(), new_orn)\n",
    "        \n",
    "#         # Simulate one step\n",
    "#         p.stepSimulation()\n",
    "#         if self.render_mode:\n",
    "#             time.sleep(0.01)\n",
    "        \n",
    "#         # Compute reward: negative reward for distance to goal, bonus for reaching the goal\n",
    "#         agent_xy = new_pos[:2]\n",
    "#         dist_to_goal = np.linalg.norm(agent_xy - self.goal)\n",
    "#         reward = -dist_to_goal\n",
    "        \n",
    "#         # Check if reached goal\n",
    "#         done = False\n",
    "#         if dist_to_goal < 0.5:\n",
    "#             reward += 100\n",
    "#             done = True\n",
    "        \n",
    "#         # Time penalty: end the episode after a fixed number of steps\n",
    "#         if self.current_step >= 100:\n",
    "#             done = True\n",
    "        \n",
    "#         return self._get_obs(), reward, done, {}\n",
    "    \n",
    "#     def render(self, mode='human'):\n",
    "#         # For GUI mode, PyBullet renders automatically.\n",
    "#         pass\n",
    "    \n",
    "#     def close(self):\n",
    "#         p.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# if __name__ == '__main__':\n",
    "#     env = DynamicBehaviorEnv(render_mode=True)\n",
    "#     obs = env.reset()\n",
    "#     for _ in range(50):\n",
    "#         action = env.action_space.sample()\n",
    "#         obs, reward, done, _ = env.step(action)\n",
    "#         if done:\n",
    "#             obs = env.reset()\n",
    "#     env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install stable-baselines3[extra]\n",
    "# %pip install 'shimmy>=2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import PPO\n",
    "\n",
    "# # Create the environment (without rendering for faster training)\n",
    "# env = DynamicBehaviorEnv(render_mode=False)\n",
    "\n",
    "# # Initialize PPO with a simple MLP policy\n",
    "# model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# # Train for a total number of timesteps (adjust as needed)\n",
    "# model.learn(total_timesteps=10000)\n",
    "\n",
    "# # Save the trained model\n",
    "# model.save(\"ppo_dynamic_behavior\")\n",
    "# print(\"RL agent trained and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import PPO\n",
    "\n",
    "# env = DynamicBehaviorEnv(render_mode=True)\n",
    "# model = PPO.load(\"ppo_dynamic_behavior\", env=env)\n",
    "\n",
    "# obs = env.reset()\n",
    "# for _ in range(100):\n",
    "#     action, _ = model.predict(obs)\n",
    "#     obs, reward, done, _ = env.step(action)\n",
    "#     env.render()  # If in GUI mode\n",
    "#     if done:\n",
    "#         obs = env.reset()\n",
    "# env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TrajectoryTransformer(nn.Module):\n",
    "#     def __init__(self, input_dim, d_model, nhead, num_layers, dropout):\n",
    "#         super(TrajectoryTransformer, self).__init__()\n",
    "#         self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers,\n",
    "#                                           num_decoder_layers=num_layers, dropout=dropout, batch_first=True) # batch_first=True is important\n",
    "#         self.fc = nn.Linear(input_dim, 2) # maps to (x,y) coordinates\n",
    "\n",
    "#     def forward(self, src, tgt): # src is the input sequence, tgt is the target sequence (for training)\n",
    "#         out = self.transformer(src, tgt)\n",
    "#         out = self.fc(out)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dim_gnn = 4  # pos_x, pos_y, v_x, v_y\n",
    "# hidden_dim_gnn = 64\n",
    "# output_dim_gnn = 64 # output dim of GNN, input dim of transformer\n",
    "# d_model_transformer = output_dim_gnn # d_model should be equal to transformer input dim\n",
    "# nhead_transformer = 8\n",
    "# num_layers_transformer = 2\n",
    "# dropout_transformer = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn_model = PedestrianGNN(input_dim_gnn, hidden_dim_gnn, output_dim_gnn)\n",
    "# transformer_model = TrajectoryTransformer(output_dim_gnn, d_model_transformer, nhead_transformer, num_layers_transformer, dropout_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PedestrianGNN(\n",
       "  (conv1): GCNConv(4, 64)\n",
       "  (conv2): GCNConv(64, 64)\n",
       "  (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrajectoryTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "# # === 3. Set up optimizer and loss function ===\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# gnn_model.to(device)\n",
    "# transformer_model.to(device)\n",
    "\n",
    "# optimizer = optim.Adam(list(gnn_model.parameters()) + list(transformer_model.parameters()), lr=1e-3)\n",
    "# criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0610, -0.0536,  0.0520,  ...,  0.0741,  0.0291, -0.1371],\n",
       "        [ 0.0511, -0.0761,  0.0104,  ...,  0.0703,  0.0424, -0.1562],\n",
       "        [ 0.0548, -0.0802,  0.0025,  ...,  0.0759,  0.0505, -0.1587],\n",
       "        ...,\n",
       "        [ 0.0547, -0.1210,  0.1108,  ...,  0.1152,  0.0978, -0.1090],\n",
       "        [ 0.0582, -0.0981,  0.0766,  ...,  0.1081,  0.0698, -0.1082],\n",
       "        [ 0.0547, -0.1210,  0.1108,  ...,  0.1152,  0.0978, -0.1090]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gnn_output = gnn_model(data_batch.x, data_batch.edge_index)\n",
    "# gnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1448, 27, 64])\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# batch_size = data_batch.batch.max().item() + 1\n",
    "# gnn_output_list = []\n",
    "\n",
    "# max_seq_len = 0  # Find the maximum sequence length across all frames\n",
    "\n",
    "# for i in range(batch_size):\n",
    "#     frame_mask = data_batch.batch == i\n",
    "#     frame_output = gnn_output[frame_mask]\n",
    "#     seq_len = frame_output.shape[0]\n",
    "#     max_seq_len = max(max_seq_len, seq_len)  # Update max_seq_len\n",
    "#     gnn_output_list.append(frame_output)  # Append the frame's features (no reshaping yet)\n",
    "\n",
    "# # Padding happens HERE, after collecting frame outputs:\n",
    "# padded_gnn_output_list = []\n",
    "# for frame_output in gnn_output_list:\n",
    "#     padding_length = max_seq_len - frame_output.shape[0]\n",
    "#     if padding_length > 0:  # Only pad if necessary\n",
    "#         padded_frame_output = torch.cat([frame_output, torch.zeros(padding_length, output_dim_gnn)], dim=0)\n",
    "#     else:\n",
    "#         padded_frame_output = frame_output # if no padding is needed, simply use the original frame output\n",
    "#     padded_gnn_output_list.append(padded_frame_output)\n",
    "\n",
    "# gnn_output = torch.stack(padded_gnn_output_list)\n",
    "# gnn_output = gnn_output.reshape(batch_size, max_seq_len, output_dim_gnn)  # Reshape after padding\n",
    "# print(gnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output_dim_gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_projection = nn.Linear(2, d_model_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = torch.randn(batch_size, max_seq_len, 2)  # (batch_size, seq_len, 2) for (x, y) coordinates\n",
    "# target_projected = target_projection(target)  # Project the target data HERE\n",
    "# transformer_output = transformer_model(gnn_output, target_projected)\n",
    "# # transformer_output = transformer_model(gnn_output, target) # Get transformer output\n",
    "# print(transformer_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0610, -0.0536,  0.0520,  ...,  0.0741,  0.0291, -0.1371],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0511, -0.0761,  0.0104,  ...,  0.0703,  0.0424, -0.1562],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0548, -0.0802,  0.0025,  ...,  0.0759,  0.0505, -0.1587],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0651, -0.1308,  0.1031,  ...,  0.1104,  0.0968, -0.1148],\n",
       "         [ 0.0651, -0.1308,  0.1031,  ...,  0.1104,  0.0968, -0.1148],\n",
       "         [ 0.0662, -0.1400,  0.0821,  ...,  0.1093,  0.0841, -0.1027],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0586, -0.1193,  0.1080,  ...,  0.1133,  0.0960, -0.1145],\n",
       "         [ 0.0586, -0.1193,  0.1080,  ...,  0.1133,  0.0960, -0.1145],\n",
       "         [ 0.0597, -0.1303,  0.0900,  ...,  0.1125,  0.0883, -0.1025],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0547, -0.1210,  0.1108,  ...,  0.1152,  0.0978, -0.1090],\n",
       "         [ 0.0547, -0.1210,  0.1108,  ...,  0.1152,  0.0978, -0.1090],\n",
       "         [ 0.0562, -0.1304,  0.0935,  ...,  0.1139,  0.0895, -0.0991],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(list(gnn_model.parameters()) + list(transformer_model.parameters()) + list(target_projection.parameters()), lr=0.001)\n",
    "# criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1448"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_samples = gnn_output.shape[0]  # Total number of samples\n",
    "# batch_size = 8  # Adjust based on GPU memory limitations\n",
    "\n",
    "# num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_nodes = data_batch.x.shape[0]\n",
    "# if (data_batch.edge_index >= num_nodes).any():\n",
    "#     print(\"ERROR: edge_index contains invalid indices!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "index 353 is out of bounds for dimension 0 with size 352",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Forward pass through GNN\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m batch_gnn_output \u001b[38;5;241m=\u001b[39m gnn_model(batch_inputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, input_dim_gnn), data_batch\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[1;32m     70\u001b[0m batch_gnn_output \u001b[38;5;241m=\u001b[39m batch_gnn_output\u001b[38;5;241m.\u001b[39mview(batch_size, max_seq_len, output_dim_gnn)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Project targets\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[36], line 11\u001b[0m, in \u001b[0;36mPedestrianGNN.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x, edge_index))\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index)) \u001b[38;5;66;03m# Add activations after conv layers\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch_geometric/nn/conv/gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m gcn_norm(  \u001b[38;5;66;03m# yapf: disable\u001b[39;00m\n\u001b[1;32m    242\u001b[0m         edge_index, edge_weight, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim),\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimproved, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_self_loops, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow, x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch_geometric/nn/conv/gcn_conv.py:108\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m row, col \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m0\u001b[39m], edge_index[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    107\u001b[0m idx \u001b[38;5;241m=\u001b[39m col \u001b[38;5;28;01mif\u001b[39;00m flow \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_to_target\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m row\n\u001b[0;32m--> 108\u001b[0m deg \u001b[38;5;241m=\u001b[39m scatter(edge_weight, idx, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dim_size\u001b[38;5;241m=\u001b[39mnum_nodes, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    109\u001b[0m deg_inv_sqrt \u001b[38;5;241m=\u001b[39m deg\u001b[38;5;241m.\u001b[39mpow_(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    110\u001b[0m deg_inv_sqrt\u001b[38;5;241m.\u001b[39mmasked_fill_(deg_inv_sqrt \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch_geometric/utils/_scatter.py:75\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     74\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mnew_zeros(size)\u001b[38;5;241m.\u001b[39mscatter_add_(dim, index, src)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     78\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: index 353 is out of bounds for dimension 0 with size 352"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # Ensure model and tensors use CUDA if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# gnn_model.to(device)\n",
    "# transformer_model.to(device)\n",
    "# target_projection.to(device)\n",
    "\n",
    "# # Training Parameters\n",
    "# num_epochs = 10  # Adjust based on GPU memory\n",
    "# batch_size = 16  # Lower if memory issues occur\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# # Prepare Dataset\n",
    "# def prepare_dataset(data_batch, future_steps=5):\n",
    "#     inputs, targets = [], []\n",
    "#     batch_size = data_batch.batch.max().item() + 1\n",
    "    \n",
    "#     for i in range(batch_size):\n",
    "#         frame_mask = data_batch.batch == i\n",
    "#         frame_data = data_batch.x[frame_mask]\n",
    "#         num_pedestrians = frame_data.shape[0]\n",
    "        \n",
    "#         if num_pedestrians < future_steps:\n",
    "#             continue  # Skip frames with too few pedestrians\n",
    "        \n",
    "#         input_seq = frame_data[:-future_steps]  # Current state\n",
    "#         target_seq = frame_data[future_steps:, :2]  # Future (x, y) positions\n",
    "        \n",
    "#         inputs.append(input_seq)\n",
    "#         targets.append(target_seq)\n",
    "    \n",
    "#     return inputs, targets\n",
    "\n",
    "# inputs, targets = prepare_dataset(data_batch)\n",
    "\n",
    "# # Pad Sequences for Transformer\n",
    "# max_seq_len = max([inp.shape[0] for inp in inputs])\n",
    "# def pad_sequences(sequences, pad_value=0):\n",
    "#     return torch.stack([torch.cat([seq, torch.full((max_seq_len - seq.shape[0], seq.shape[1]), pad_value)]) for seq in sequences])\n",
    "\n",
    "# inputs_padded = pad_sequences(inputs)\n",
    "# targets_padded = pad_sequences(targets)\n",
    "\n",
    "# # Move to device\n",
    "# inputs_padded = inputs_padded.to(device)\n",
    "# targets_padded = targets_padded.to(device)\n",
    "\n",
    "# # Create DataLoader\n",
    "# dataset = TensorDataset(inputs_padded, targets_padded)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Define Optimizer & Loss Function\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     list(gnn_model.parameters()) + list(transformer_model.parameters()) + list(target_projection.parameters()),\n",
    "#     lr=learning_rate\n",
    "# )\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# # Training Loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     total_loss = 0\n",
    "#     for batch_inputs, batch_targets in dataloader:\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Forward pass through GNN\n",
    "#         batch_gnn_output = gnn_model(batch_inputs.view(-1, input_dim_gnn), data_batch.edge_index)\n",
    "#         batch_gnn_output = batch_gnn_output.view(batch_size, max_seq_len, output_dim_gnn)\n",
    "        \n",
    "#         # Project targets\n",
    "#         batch_targets_projected = target_projection(batch_targets)\n",
    "        \n",
    "#         # Forward pass through Transformer\n",
    "#         transformer_output = transformer_model(batch_gnn_output, batch_targets_projected)\n",
    "        \n",
    "#         # Compute loss\n",
    "#         loss = criterion(transformer_output, batch_targets_projected)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# # Testing\n",
    "# with torch.no_grad():\n",
    "#     gnn_output_test = gnn_model(inputs_padded.view(-1, input_dim_gnn), data_batch.edge_index)\n",
    "#     gnn_output_test = gnn_output_test.view(batch_size, max_seq_len, output_dim_gnn)\n",
    "#     target_test_projected = target_projection(targets_padded)\n",
    "#     transformer_output_test = transformer_model(gnn_output_test, target_test_projected)\n",
    "#     test_loss = criterion(transformer_output_test, target_test_projected)\n",
    "#     print(f\"Test Loss: {test_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "the batch number of src and tgt must be equal",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, max_seq_len, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (batch_size, seq_len, 2) for (x, y) coordinates\u001b[39;00m\n\u001b[1;32m      2\u001b[0m target_projected \u001b[38;5;241m=\u001b[39m target_projection(target)  \u001b[38;5;66;03m# Project the target data HERE\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m transformer_output \u001b[38;5;241m=\u001b[39m transformer_model(gnn_output, target_projected)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# transformer_output = transformer_model(gnn_output, target) # Get transformer output\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(transformer_output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[37], line 9\u001b[0m, in \u001b[0;36mTrajectoryTransformer.forward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt): \u001b[38;5;66;03m# src is the input sequence, tgt is the target sequence (for training)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(src, tgt)\n\u001b[1;32m     10\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:265\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m!=\u001b[39m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;129;01mor\u001b[39;00m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: the batch number of src and tgt must be equal"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# # Assuming gnn_output and target are tensors of shape (num_samples, max_seq_len, feature_dim)\n",
    "# num_samples = gnn_output.shape[0]  # Total number of samples\n",
    "# batch_size = 8  # Adjust based on GPU memory limitations\n",
    "\n",
    "# # Prepare dataset and DataLoader\n",
    "# dataset = TensorDataset(gnn_output, target_projected)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Training loop\n",
    "# def train_model(gnn_model, transformer_model, target_projection, dataloader, optimizer, criterion, num_epochs=10):\n",
    "#     gnn_model.train()\n",
    "#     transformer_model.train()\n",
    "#     target_projection.train()\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss = 0\n",
    "#         for batch in dataloader:\n",
    "#             gnn_inputs, target = batch\n",
    "#             gnn_inputs, target = gnn_inputs.cuda(), target.cuda()\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             gnn_output = gnn_model(gnn_inputs.reshape(-1, output_dim_gnn), data_batch.edge_index)\n",
    "#             gnn_output = gnn_output.reshape(batch_size, max_seq_len, output_dim_gnn)\n",
    "            \n",
    "#             transformer_output = transformer_model(gnn_output, target)\n",
    "            \n",
    "#             loss = criterion(transformer_output, target)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             total_loss += loss.item()\n",
    "        \n",
    "#         print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# # Testing loop\n",
    "# def test_model(gnn_model, transformer_model, target_projection, dataloader, criterion):\n",
    "#     gnn_model.eval()\n",
    "#     transformer_model.eval()\n",
    "#     target_projection.eval()\n",
    "    \n",
    "#     total_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataloader:\n",
    "#             gnn_inputs, target = batch\n",
    "#             gnn_inputs, target = gnn_inputs.cuda(), target.cuda()\n",
    "            \n",
    "#             gnn_output = gnn_model(gnn_inputs.reshape(-1, output_dim_gnn), data_batch.edge_index)\n",
    "#             gnn_output = gnn_output.reshape(batch_size, max_seq_len, output_dim_gnn)\n",
    "            \n",
    "#             transformer_output = transformer_model(gnn_output, target)\n",
    "            \n",
    "#             loss = criterion(transformer_output, target)\n",
    "#             total_loss += loss.item()\n",
    "    \n",
    "#     print(f\"Test Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# # Move models to GPU\n",
    "# gnn_model = gnn_model.cuda()\n",
    "# transformer_model = transformer_model.cuda()\n",
    "# target_projection = target_projection.cuda()\n",
    "\n",
    "# # Train and test\n",
    "# train_model(gnn_model, transformer_model, target_projection, dataloader, optimizer, criterion, num_epochs=10)\n",
    "# test_model(gnn_model, transformer_model, target_projection, dataloader, criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
